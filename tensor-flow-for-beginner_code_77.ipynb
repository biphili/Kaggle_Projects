{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sonar.all-data.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import \n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets print Hello, Tensorflow using TF program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "b'Hello,tensorflow!'\n"
     ]
    }
   ],
   "source": [
    "# Define a hello tensor as a tf.constant\n",
    "hello=tf.constant(\"Hello,tensorflow!\")\n",
    "print(hello)\n",
    "\n",
    "#Define a TF session\n",
    "sess=tf.Session()\n",
    "\n",
    "# Run the hello tensor under the session \n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining tensors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Deine cosstant Scalar \n",
    "ex_tensor=tf.constant(3)\n",
    "tf.shape(ex_tensor)\n",
    "tf.rank(ex_tensor)\n",
    "print(ex_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Defining a List\n",
    "ex_tensor=tf.constant([1,2,3])\n",
    "#print(tf.shape(ex_tensor))\n",
    "#tf.rank(ex_tensor)\n",
    "print(ex_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Defining a matrix \n",
    "ex_tensor=tf.constant([[1,2,3],[4,5,6]])\n",
    "tf.shape(ex_tensor)\n",
    "tf.rank(ex_tensor)\n",
    "print(ex_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_4:0\", shape=(2, 1, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Defining a 3d array \n",
    "ex_tensor=tf.constant([[[1,2,3]],[[7,8,9]]])\n",
    "tf.shape(ex_tensor)\n",
    "tf.rank(ex_tensor)\n",
    "print(ex_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant(10)\n",
    "b=tf.constant(20)\n",
    "#a+b\n",
    "with tf.Session() as sess:\n",
    "    result=sess.run(a+b)   \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "const=tf.constant(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Fill:0' shape=(4, 4) dtype=int32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mat=tf.fill((4,4),10) # Tensor filled matrix \n",
    "fill_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myzeros=tf.zeros((4,4)) # Created 4x4 Zero tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myones=tf.ones((4,4))# Creates 4x4 one tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrandn=tf.random_normal((4,4),mean=0,stddev=1.0) # Creates a random normal distribution tensor with mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrandu=tf.random_uniform((4,4),minval=0,maxval=1) #Creates a unifrom distribution tensor with minimum value as 0 and maximum value as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "\n",
      "[[10 10 10 10]\n",
      " [10 10 10 10]\n",
      " [10 10 10 10]\n",
      " [10 10 10 10]]\n",
      "\n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "\n",
      "[[-0.18039273  0.9844249  -0.23699181  0.8850605 ]\n",
      " [ 0.15957987  1.0458542   1.0272535   0.14756541]\n",
      " [ 0.7849239   0.65393776  1.0763698  -2.3218594 ]\n",
      " [ 0.14309241 -0.6045882   0.29696065 -1.2442594 ]]\n",
      "\n",
      "\n",
      "[[0.13608563 0.48987877 0.6533747  0.32043862]\n",
      " [0.6497365  0.40992117 0.8621942  0.5063715 ]\n",
      " [0.14976525 0.12264693 0.230672   0.923053  ]\n",
      " [0.9605268  0.73436534 0.04699492 0.10292447]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prenting all the results \n",
    "my_ops=[const,fill_mat,myzeros,myones,myrandn,myrandu]\n",
    "sess=tf.InteractiveSession() # INteractive Session helps to get the result out of the Session\n",
    "for op in my_ops:\n",
    "    print(sess.run(op))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "a.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(1)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=tf.constant([[10],[100]])\n",
    "b.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[210],\n",
       "       [430]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=tf.matmul(a,b)\n",
    "sess.run(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[210],\n",
       "       [430]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF Core Progrma Structure **\n",
    "\n",
    "Tensorflow core program consists of two discrete sections:\n",
    "\n",
    "1.Building the computational graph(a.tf.Graph)\n",
    "\n",
    "2.Running the computational graph (using a tf.Session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph**\n",
    "\n",
    "A computational graph is a series of Tensorflow operations arranged into a graph.The graph is composed of two typed of objects.\n",
    "\n",
    "**1.Operations**(or \"ops\"):The nodes of the graph.Operations describe calculations that consume and produce tensors\n",
    "\n",
    "**2.Tensors**:The edges in the graph.These represent the valuess that will flow through the graph.Most TensorFlow functions return tf.Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_10:0\", shape=(2,), dtype=float32)\n",
      "Tensor(\"Const_11:0\", shape=(2,), dtype=float32)\n",
      "Tensor(\"add_1:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant([3.0,2.0],dtype=tf.float32)\n",
    "b=tf.constant([4.0,1.0])  #also tf.float32 implicitly \n",
    "total=a+b\n",
    "print(a)\n",
    "print(b)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Session**\n",
    "\n",
    "To evaluate tensors,instantiate a tf.Session object,informally known as a session.A session encapsulate the state of the Tensorflow runtime and runs tensorflow operations.If a tf.Graph is like a .py file a tf.Session is like the python executable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 3.]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "print(sess.run(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Placeholders**\n",
    "\n",
    "A graph can be parameterized to accept external inputs,known as placeholders.A placeholder is a promise to provide a valaue later like a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 8.]\n"
     ]
    }
   ],
   "source": [
    "#Build a graph \n",
    "x=tf.placeholder(tf.float32)\n",
    "y=tf.placeholder(tf.float32)\n",
    "z=x+y\n",
    "\n",
    "#Define session for executing the graph \n",
    "# feed_dict argument of the run method to feed concrete values \n",
    "print(sess.run(z,feed_dict={x:3,y:4.5}))\n",
    "print(sess.run(z,feed_dict={x:[1,3],y:[2,5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the data set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    df=pd.read_csv('../input/sonar.all-data.csv')\n",
    "   # print(len(df.columns))\n",
    "    X=df[df.columns[0:60]].values\n",
    "    y=df[df.columns[60]]\n",
    "#Encode the independent variable \n",
    "    encoder=LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    Y=one_hot_encode(y)\n",
    "    print(X.shape)\n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the encoder function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    n_labels=len(labels)\n",
    "    n_unique_labels=len(np.unique(labels))\n",
    "    one_hot_encode=np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels),labels]=1\n",
    "    return one_hot_encode  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207, 60)\n"
     ]
    }
   ],
   "source": [
    "X,Y=read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S**huffle the data to mix up rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=shuffle(X,Y,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the data into train and test set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.2,random_state=415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting the shape of training and the test data set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 60)\n",
      "(165, 2)\n",
      "(42, 60)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the important parameters to work with tensors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_dim 60\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.3\n",
    "training_epochs=1000\n",
    "cost_history=np.empty(shape=[1],dtype=float)\n",
    "n_dim=X.shape[1]\n",
    "print('n_dim',n_dim)\n",
    "n_class=2\n",
    "#model_path=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the number of hidden layers and number of neurons for each layer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1=60\n",
    "n_hidden_2=60\n",
    "n_hidden_3=60\n",
    "n_hidden_4=60\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,n_dim])\n",
    "W=tf.Variable(tf.zeros([n_dim,n_class]))\n",
    "b=tf.Variable(tf.zeros([n_class]))\n",
    "y_=tf.placeholder(tf.float32,[None,n_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_preceptron(x,weights,biases):\n",
    "    # Hidden layer with Relu activised \n",
    "    \n",
    "    layer_1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1=tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    # Hidden layer with Sigmoid activation \n",
    "    layer_2=tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2=tf.nn.sigmoid(layer_2)\n",
    "    \n",
    "     # Hidden layer with Sigmoid activation \n",
    "    layer_3=tf.add(tf.matmul(layer_2,weights['h3']),biases['b3'])\n",
    "    layer_3=tf.nn.sigmoid(layer_3)\n",
    "    \n",
    "     # Hidden layer with Relu activation \n",
    "    layer_4=tf.add(tf.matmul(layer_3,weights['h4']),biases['b4'])\n",
    "    layer_4=tf.nn.sigmoid(layer_4)\n",
    "    \n",
    "    # Output layer with Linear Activation \n",
    "    out_layer=tf.matmul(layer_4,weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the weights and the biases for each layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={\n",
    "    'h1':tf.Variable(tf.truncated_normal([n_dim,n_hidden_1])),\n",
    "    'h2':tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
    "    'h3':tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3])),\n",
    "    'h4':tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4])),\n",
    "    'out':tf.Variable(tf.truncated_normal([n_hidden_4,n_class])),\n",
    "}\n",
    "\n",
    "biases={\n",
    "    'b1':tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "    'b3':tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "    'b4':tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "    'out':tf.Variable(tf.truncated_normal([n_class])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initalise all the variables **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call the defined model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=multilayer_preceptron(x,weights,biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the cost function and the optimizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_))\n",
    "training_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the coat and the accuracy of each epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  -  cost: 1.0915358 - MSE:  2.5893301524845915 -Train Accuracy: 0.45454547\n",
      "epoch: 1  -  cost: 2.9316285 - MSE:  11.36802266366423 -Train Accuracy: 0.54545456\n",
      "epoch: 2  -  cost: 1.5435524 - MSE:  3.387004126887901 -Train Accuracy: 0.45454547\n",
      "epoch: 3  -  cost: 2.5012755 - MSE:  8.491931021543543 -Train Accuracy: 0.54545456\n",
      "epoch: 4  -  cost: 1.1085469 - MSE:  2.333474716043514 -Train Accuracy: 0.45454547\n",
      "epoch: 5  -  cost: 2.0857525 - MSE:  6.502668781229051 -Train Accuracy: 0.54545456\n",
      "epoch: 6  -  cost: 1.3223813 - MSE:  2.983197860382237 -Train Accuracy: 0.45454547\n",
      "epoch: 7  -  cost: 2.1258833 - MSE:  6.870364304224059 -Train Accuracy: 0.54545456\n",
      "epoch: 8  -  cost: 1.1908176 - MSE:  2.7784402458519097 -Train Accuracy: 0.45454547\n",
      "epoch: 9  -  cost: 1.9651395 - MSE:  6.187599121054197 -Train Accuracy: 0.54545456\n",
      "epoch: 10  -  cost: 1.2644957 - MSE:  3.0781020744962873 -Train Accuracy: 0.45454547\n",
      "epoch: 11  -  cost: 1.9411808 - MSE:  6.183192215564188 -Train Accuracy: 0.54545456\n",
      "epoch: 12  -  cost: 1.2370037 - MSE:  3.129889813496985 -Train Accuracy: 0.45454547\n",
      "epoch: 13  -  cost: 1.8795125 - MSE:  6.006999336047461 -Train Accuracy: 0.54545456\n",
      "epoch: 14  -  cost: 1.2485983 - MSE:  3.283167747872767 -Train Accuracy: 0.45454547\n",
      "epoch: 15  -  cost: 1.845245 - MSE:  5.9728464637311145 -Train Accuracy: 0.54545456\n",
      "epoch: 16  -  cost: 1.2436404 - MSE:  3.4055322305769433 -Train Accuracy: 0.45454547\n",
      "epoch: 17  -  cost: 1.8109075 - MSE:  5.9544320945018425 -Train Accuracy: 0.54545456\n",
      "epoch: 18  -  cost: 1.2422394 - MSE:  3.5478664722186837 -Train Accuracy: 0.45454547\n",
      "epoch: 19  -  cost: 1.7827973 - MSE:  5.980813131455483 -Train Accuracy: 0.54545456\n",
      "epoch: 20  -  cost: 1.2386535 - MSE:  3.6962496840627312 -Train Accuracy: 0.45454547\n",
      "epoch: 21  -  cost: 1.7574387 - MSE:  6.035394436717069 -Train Accuracy: 0.54545456\n",
      "epoch: 22  -  cost: 1.2349712 - MSE:  3.8548496819584623 -Train Accuracy: 0.45454547\n",
      "epoch: 23  -  cost: 1.7347738 - MSE:  6.115992319842007 -Train Accuracy: 0.54545456\n",
      "epoch: 24  -  cost: 1.2309054 - MSE:  4.021716897867591 -Train Accuracy: 0.45454547\n",
      "epoch: 25  -  cost: 1.7141194 - MSE:  6.217390518004576 -Train Accuracy: 0.54545456\n",
      "epoch: 26  -  cost: 1.2267123 - MSE:  4.1963277311919045 -Train Accuracy: 0.45454547\n",
      "epoch: 27  -  cost: 1.6951407 - MSE:  6.336215422280036 -Train Accuracy: 0.54545456\n",
      "epoch: 28  -  cost: 1.2224311 - MSE:  4.377928054669526 -Train Accuracy: 0.45454547\n",
      "epoch: 29  -  cost: 1.6775309 - MSE:  6.469888155111544 -Train Accuracy: 0.54545456\n",
      "epoch: 30  -  cost: 1.2181028 - MSE:  4.56592047265158 -Train Accuracy: 0.45454547\n",
      "epoch: 31  -  cost: 1.6610779 - MSE:  6.616766237443678 -Train Accuracy: 0.54545456\n",
      "epoch: 32  -  cost: 1.2137458 - MSE:  4.759478021926216 -Train Accuracy: 0.45454547\n",
      "epoch: 33  -  cost: 1.6456503 - MSE:  6.775600699496617 -Train Accuracy: 0.54545456\n",
      "epoch: 34  -  cost: 1.2093908 - MSE:  4.957331165148775 -Train Accuracy: 0.45454547\n",
      "epoch: 35  -  cost: 1.6311964 - MSE:  6.944949338428038 -Train Accuracy: 0.54545456\n",
      "epoch: 36  -  cost: 1.2050886 - MSE:  5.157711960112912 -Train Accuracy: 0.45454547\n",
      "epoch: 37  -  cost: 1.6177231 - MSE:  7.122752406065803 -Train Accuracy: 0.54545456\n",
      "epoch: 38  -  cost: 1.2009085 - MSE:  5.3585968768052386 -Train Accuracy: 0.45454547\n",
      "epoch: 39  -  cost: 1.6052568 - MSE:  7.306313011207681 -Train Accuracy: 0.54545456\n",
      "epoch: 40  -  cost: 1.1969227 - MSE:  5.5581369649067005 -Train Accuracy: 0.45454547\n",
      "epoch: 41  -  cost: 1.5938016 - MSE:  7.49270602174834 -Train Accuracy: 0.54545456\n",
      "epoch: 42  -  cost: 1.1931828 - MSE:  5.755072539421693 -Train Accuracy: 0.45454547\n",
      "epoch: 43  -  cost: 1.583318 - MSE:  7.679401460010798 -Train Accuracy: 0.54545456\n",
      "epoch: 44  -  cost: 1.1897097 - MSE:  5.948977466692206 -Train Accuracy: 0.45454547\n",
      "epoch: 45  -  cost: 1.573717 - MSE:  7.8647569872978345 -Train Accuracy: 0.54545456\n",
      "epoch: 46  -  cost: 1.1864927 - MSE:  6.140250790868092 -Train Accuracy: 0.45454547\n",
      "epoch: 47  -  cost: 1.5648752 - MSE:  8.048214049510669 -Train Accuracy: 0.54545456\n",
      "epoch: 48  -  cost: 1.1835002 - MSE:  6.329964557961337 -Train Accuracy: 0.45454547\n",
      "epoch: 49  -  cost: 1.5566545 - MSE:  8.23023266136194 -Train Accuracy: 0.54545456\n",
      "epoch: 50  -  cost: 1.1806861 - MSE:  6.51963990212042 -Train Accuracy: 0.45454547\n",
      "epoch: 51  -  cost: 1.5489157 - MSE:  8.412070031013362 -Train Accuracy: 0.54545456\n",
      "epoch: 52  -  cost: 1.1780022 - MSE:  6.7110320610915055 -Train Accuracy: 0.45454547\n",
      "epoch: 53  -  cost: 1.5415308 - MSE:  8.595532409289506 -Train Accuracy: 0.54545456\n",
      "epoch: 54  -  cost: 1.1753999 - MSE:  6.905950885738551 -Train Accuracy: 0.45454547\n",
      "epoch: 55  -  cost: 1.5343869 - MSE:  8.782754633323558 -Train Accuracy: 0.54545456\n",
      "epoch: 56  -  cost: 1.1728336 - MSE:  7.106092138477576 -Train Accuracy: 0.45454547\n",
      "epoch: 57  -  cost: 1.5273885 - MSE:  8.97599853302058 -Train Accuracy: 0.54545456\n",
      "epoch: 58  -  cost: 1.170263 - MSE:  7.312887820666173 -Train Accuracy: 0.45454547\n",
      "epoch: 59  -  cost: 1.5204616 - MSE:  9.177475636122637 -Train Accuracy: 0.54545456\n",
      "epoch: 60  -  cost: 1.1676538 - MSE:  7.527334294839015 -Train Accuracy: 0.45454547\n",
      "epoch: 61  -  cost: 1.5135542 - MSE:  9.38911534578745 -Train Accuracy: 0.54545456\n",
      "epoch: 62  -  cost: 1.1649799 - MSE:  7.749808177958357 -Train Accuracy: 0.45454547\n",
      "epoch: 63  -  cost: 1.5066404 - MSE:  9.612301191690884 -Train Accuracy: 0.54545456\n",
      "epoch: 64  -  cost: 1.1622267 - MSE:  7.979905073497851 -Train Accuracy: 0.45454547\n",
      "epoch: 65  -  cost: 1.4997228 - MSE:  9.847576570629831 -Train Accuracy: 0.54545456\n",
      "epoch: 66  -  cost: 1.1593931 - MSE:  8.216307435885149 -Train Accuracy: 0.45454547\n",
      "epoch: 67  -  cost: 1.492832 - MSE:  10.094341811304902 -Train Accuracy: 0.54545456\n",
      "epoch: 68  -  cost: 1.1564926 - MSE:  8.456777880700702 -Train Accuracy: 0.45454547\n",
      "epoch: 69  -  cost: 1.4860232 - MSE:  10.350676514307898 -Train Accuracy: 0.54545456\n",
      "epoch: 70  -  cost: 1.1535552 - MSE:  8.698304439133938 -Train Accuracy: 0.45454547\n",
      "epoch: 71  -  cost: 1.4793675 - MSE:  10.61337394152501 -Train Accuracy: 0.54545456\n",
      "epoch: 72  -  cost: 1.1506184 - MSE:  8.93742536573899 -Train Accuracy: 0.45454547\n",
      "epoch: 73  -  cost: 1.4729377 - MSE:  10.878238449325616 -Train Accuracy: 0.54545456\n",
      "epoch: 74  -  cost: 1.1477251 - MSE:  9.170651913532687 -Train Accuracy: 0.45454547\n",
      "epoch: 75  -  cost: 1.4667946 - MSE:  11.140640000807574 -Train Accuracy: 0.54545456\n",
      "epoch: 76  -  cost: 1.144912 - MSE:  9.39489305897624 -Train Accuracy: 0.45454547\n",
      "epoch: 77  -  cost: 1.4609785 - MSE:  11.39615260070642 -Train Accuracy: 0.54545456\n",
      "epoch: 78  -  cost: 1.1422043 - MSE:  9.607765196750407 -Train Accuracy: 0.45454547\n",
      "epoch: 79  -  cost: 1.4555 - MSE:  11.641075388140456 -Train Accuracy: 0.54545456\n",
      "epoch: 80  -  cost: 1.1396137 - MSE:  9.807734002624159 -Train Accuracy: 0.45454547\n",
      "epoch: 81  -  cost: 1.450346 - MSE:  11.872758758596476 -Train Accuracy: 0.54545456\n",
      "epoch: 82  -  cost: 1.137138 - MSE:  9.994084489200779 -Train Accuracy: 0.45454547\n",
      "epoch: 83  -  cost: 1.4454826 - MSE:  12.089633279614258 -Train Accuracy: 0.54545456\n",
      "epoch: 84  -  cost: 1.1347667 - MSE:  10.166783860411025 -Train Accuracy: 0.45454547\n",
      "epoch: 85  -  cost: 1.4408665 - MSE:  12.291100007139287 -Train Accuracy: 0.54545456\n",
      "epoch: 86  -  cost: 1.1324809 - MSE:  10.326287703043281 -Train Accuracy: 0.45454547\n",
      "epoch: 87  -  cost: 1.4364474 - MSE:  12.4772753520799 -Train Accuracy: 0.54545456\n",
      "epoch: 88  -  cost: 1.13026 - MSE:  10.473369666690743 -Train Accuracy: 0.45454547\n",
      "epoch: 89  -  cost: 1.4321761 - MSE:  12.648766342417554 -Train Accuracy: 0.54545456\n",
      "epoch: 90  -  cost: 1.1280824 - MSE:  10.608963897523806 -Train Accuracy: 0.45454547\n",
      "epoch: 91  -  cost: 1.4280075 - MSE:  12.806474667242496 -Train Accuracy: 0.54545456\n",
      "epoch: 92  -  cost: 1.1259269 - MSE:  10.734072306576374 -Train Accuracy: 0.45454547\n",
      "epoch: 93  -  cost: 1.4239006 - MSE:  12.951419101405726 -Train Accuracy: 0.54545456\n",
      "epoch: 94  -  cost: 1.123774 - MSE:  10.8496763660997 -Train Accuracy: 0.45454547\n",
      "epoch: 95  -  cost: 1.41982 - MSE:  13.084657241269033 -Train Accuracy: 0.54545456\n",
      "epoch: 96  -  cost: 1.1216068 - MSE:  10.956710183562626 -Train Accuracy: 0.45454547\n",
      "epoch: 97  -  cost: 1.4157369 - MSE:  13.207207386988424 -Train Accuracy: 0.54545456\n",
      "epoch: 98  -  cost: 1.1194091 - MSE:  11.056015255223619 -Train Accuracy: 0.45454547\n",
      "epoch: 99  -  cost: 1.4116267 - MSE:  13.320015206142054 -Train Accuracy: 0.54545456\n",
      "epoch: 100  -  cost: 1.1171672 - MSE:  11.14836445170032 -Train Accuracy: 0.45454547\n",
      "epoch: 101  -  cost: 1.4074687 - MSE:  13.423948163435531 -Train Accuracy: 0.54545456\n",
      "epoch: 102  -  cost: 1.1148697 - MSE:  11.234433198149121 -Train Accuracy: 0.45454547\n",
      "epoch: 103  -  cost: 1.4032477 - MSE:  13.519774180582104 -Train Accuracy: 0.54545456\n",
      "epoch: 104  -  cost: 1.1125057 - MSE:  11.31481320432099 -Train Accuracy: 0.45454547\n",
      "epoch: 105  -  cost: 1.3989503 - MSE:  13.60817514667236 -Train Accuracy: 0.54545456\n",
      "epoch: 106  -  cost: 1.1100676 - MSE:  11.390017452959516 -Train Accuracy: 0.45454547\n",
      "epoch: 107  -  cost: 1.3945675 - MSE:  13.689752445960309 -Train Accuracy: 0.54545456\n",
      "epoch: 108  -  cost: 1.1075482 - MSE:  11.460494446520263 -Train Accuracy: 0.45454547\n",
      "epoch: 109  -  cost: 1.3900918 - MSE:  13.76503241552737 -Train Accuracy: 0.54545456\n",
      "epoch: 110  -  cost: 1.1049421 - MSE:  11.526626454979954 -Train Accuracy: 0.45454547\n",
      "epoch: 111  -  cost: 1.3855178 - MSE:  13.83447072785314 -Train Accuracy: 0.54545456\n",
      "epoch: 112  -  cost: 1.1022456 - MSE:  11.588740591689357 -Train Accuracy: 0.45454547\n",
      "epoch: 113  -  cost: 1.380842 - MSE:  13.898468904932987 -Train Accuracy: 0.54545456\n",
      "epoch: 114  -  cost: 1.0994569 - MSE:  11.647116373516422 -Train Accuracy: 0.45454547\n",
      "epoch: 115  -  cost: 1.3760628 - MSE:  13.957372867658908 -Train Accuracy: 0.54545456\n",
      "epoch: 116  -  cost: 1.0965738 - MSE:  11.701986341629244 -Train Accuracy: 0.45454547\n",
      "epoch: 117  -  cost: 1.3711784 - MSE:  14.011479995122958 -Train Accuracy: 0.54545456\n",
      "epoch: 118  -  cost: 1.0935972 - MSE:  11.753553606919525 -Train Accuracy: 0.45454547\n",
      "epoch: 119  -  cost: 1.3661877 - MSE:  14.06104928887386 -Train Accuracy: 0.54545456\n",
      "epoch: 120  -  cost: 1.0905278 - MSE:  11.801980503053816 -Train Accuracy: 0.45454547\n",
      "epoch: 121  -  cost: 1.3610911 - MSE:  14.106299266177697 -Train Accuracy: 0.54545456\n",
      "epoch: 122  -  cost: 1.0873668 - MSE:  11.847401760943775 -Train Accuracy: 0.45454547\n",
      "epoch: 123  -  cost: 1.3558865 - MSE:  14.147408692793906 -Train Accuracy: 0.54545456\n",
      "epoch: 124  -  cost: 1.0841167 - MSE:  11.889923433406947 -Train Accuracy: 0.45454547\n",
      "epoch: 125  -  cost: 1.350573 - MSE:  14.184526069826951 -Train Accuracy: 0.54545456\n",
      "epoch: 126  -  cost: 1.0807797 - MSE:  11.929622910285515 -Train Accuracy: 0.45454547\n",
      "epoch: 127  -  cost: 1.3451477 - MSE:  14.217759818876171 -Train Accuracy: 0.54545456\n",
      "epoch: 128  -  cost: 1.0773579 - MSE:  11.966552557071834 -Train Accuracy: 0.45454547\n",
      "epoch: 129  -  cost: 1.3396068 - MSE:  14.247185917721941 -Train Accuracy: 0.54545456\n",
      "epoch: 130  -  cost: 1.0738533 - MSE:  12.000741621097049 -Train Accuracy: 0.45454547\n",
      "epoch: 131  -  cost: 1.3339446 - MSE:  14.272842852765049 -Train Accuracy: 0.54545456\n",
      "epoch: 132  -  cost: 1.0702671 - MSE:  12.032184384599978 -Train Accuracy: 0.45454547\n",
      "epoch: 133  -  cost: 1.3281529 - MSE:  14.294732542134208 -Train Accuracy: 0.54545456\n",
      "epoch: 134  -  cost: 1.0665997 - MSE:  12.06086081431831 -Train Accuracy: 0.45454547\n",
      "epoch: 135  -  cost: 1.3222226 - MSE:  14.312817319876185 -Train Accuracy: 0.54545456\n",
      "epoch: 136  -  cost: 1.06285 - MSE:  12.086716161789989 -Train Accuracy: 0.45454547\n",
      "epoch: 137  -  cost: 1.3161404 - MSE:  14.32701710002689 -Train Accuracy: 0.54545456\n",
      "epoch: 138  -  cost: 1.0590162 - MSE:  12.109667065967372 -Train Accuracy: 0.45454547\n",
      "epoch: 139  -  cost: 1.3098911 - MSE:  14.337204317101973 -Train Accuracy: 0.54545456\n",
      "epoch: 140  -  cost: 1.0550942 - MSE:  12.129606341591781 -Train Accuracy: 0.45454547\n",
      "epoch: 141  -  cost: 1.3034567 - MSE:  14.343210390753892 -Train Accuracy: 0.54545456\n",
      "epoch: 142  -  cost: 1.0510776 - MSE:  12.146399242219438 -Train Accuracy: 0.45454547\n",
      "epoch: 143  -  cost: 1.2968155 - MSE:  14.344816031229621 -Train Accuracy: 0.54545456\n",
      "epoch: 144  -  cost: 1.0469589 - MSE:  12.159894220980604 -Train Accuracy: 0.45454547\n",
      "epoch: 145  -  cost: 1.2899437 - MSE:  14.341759234516266 -Train Accuracy: 0.55757576\n",
      "epoch: 146  -  cost: 1.0427269 - MSE:  12.169922415430088 -Train Accuracy: 0.45454547\n",
      "epoch: 147  -  cost: 1.2828152 - MSE:  14.333735206483373 -Train Accuracy: 0.55757576\n",
      "epoch: 148  -  cost: 1.0383689 - MSE:  12.176305978497949 -Train Accuracy: 0.45454547\n",
      "epoch: 149  -  cost: 1.2754006 - MSE:  14.320402739780729 -Train Accuracy: 0.55757576\n",
      "epoch: 150  -  cost: 1.0338697 - MSE:  12.178891542146024 -Train Accuracy: 0.45454547\n",
      "epoch: 151  -  cost: 1.2676716 - MSE:  14.30141204742263 -Train Accuracy: 0.55757576\n",
      "epoch: 152  -  cost: 1.0292118 - MSE:  12.17756286753729 -Train Accuracy: 0.45454547\n",
      "epoch: 153  -  cost: 1.2595992 - MSE:  14.276434422447144 -Train Accuracy: 0.56363636\n",
      "epoch: 154  -  cost: 1.0243763 - MSE:  12.172284659606223 -Train Accuracy: 0.45454547\n",
      "epoch: 155  -  cost: 1.2511592 - MSE:  14.24519876246149 -Train Accuracy: 0.56363636\n",
      "epoch: 156  -  cost: 1.0193442 - MSE:  12.163138041785581 -Train Accuracy: 0.45454547\n",
      "epoch: 157  -  cost: 1.2423322 - MSE:  14.207560476611842 -Train Accuracy: 0.56363636\n",
      "epoch: 158  -  cost: 1.0140988 - MSE:  12.150388238198664 -Train Accuracy: 0.45454547\n",
      "epoch: 159  -  cost: 1.2331096 - MSE:  14.163587923213626 -Train Accuracy: 0.56363636\n",
      "epoch: 160  -  cost: 1.008627 - MSE:  12.134520666663901 -Train Accuracy: 0.45454547\n",
      "epoch: 161  -  cost: 1.2234955 - MSE:  14.113650245607547 -Train Accuracy: 0.56363636\n",
      "epoch: 162  -  cost: 1.0029235 - MSE:  12.116299195556662 -Train Accuracy: 0.45454547\n",
      "epoch: 163  -  cost: 1.213512 - MSE:  14.058500011847931 -Train Accuracy: 0.56363636\n",
      "epoch: 164  -  cost: 0.9969943 - MSE:  12.09676025284223 -Train Accuracy: 0.45454547\n",
      "epoch: 165  -  cost: 1.2032 - MSE:  13.99934364017882 -Train Accuracy: 0.56363636\n",
      "epoch: 166  -  cost: 0.99085695 - MSE:  12.077197319296992 -Train Accuracy: 0.45454547\n",
      "epoch: 167  -  cost: 1.1926191 - MSE:  13.937817027831313 -Train Accuracy: 0.56363636\n",
      "epoch: 168  -  cost: 0.98454607 - MSE:  12.059071119410058 -Train Accuracy: 0.45454547\n",
      "epoch: 169  -  cost: 1.1818461 - MSE:  13.875905178714808 -Train Accuracy: 0.56363636\n",
      "epoch: 170  -  cost: 0.97810835 - MSE:  12.043872151631318 -Train Accuracy: 0.45454547\n",
      "epoch: 171  -  cost: 1.1709679 - MSE:  13.815745945649192 -Train Accuracy: 0.56969696\n",
      "epoch: 172  -  cost: 0.97160137 - MSE:  12.032964613669932 -Train Accuracy: 0.45454547\n",
      "epoch: 173  -  cost: 1.1600777 - MSE:  13.759394586847586 -Train Accuracy: 0.56969696\n",
      "epoch: 174  -  cost: 0.9650894 - MSE:  12.027422681652737 -Train Accuracy: 0.45454547\n",
      "epoch: 175  -  cost: 1.1492678 - MSE:  13.708581464918945 -Train Accuracy: 0.56969696\n",
      "epoch: 176  -  cost: 0.9586376 - MSE:  12.027923546204672 -Train Accuracy: 0.45454547\n",
      "epoch: 177  -  cost: 1.1386288 - MSE:  13.664532224014037 -Train Accuracy: 0.57575756\n",
      "epoch: 178  -  cost: 0.95230836 - MSE:  12.034673253809151 -Train Accuracy: 0.45454547\n",
      "epoch: 179  -  cost: 1.128247 - MSE:  13.62789213634267 -Train Accuracy: 0.57575756\n",
      "epoch: 180  -  cost: 0.9461569 - MSE:  12.04741665412197 -Train Accuracy: 0.45454547\n",
      "epoch: 181  -  cost: 1.1182027 - MSE:  13.598750148092703 -Train Accuracy: 0.57575756\n",
      "epoch: 182  -  cost: 0.9402292 - MSE:  12.065511201050342 -Train Accuracy: 0.45454547\n",
      "epoch: 183  -  cost: 1.1085706 - MSE:  13.576748731201773 -Train Accuracy: 0.58181816\n",
      "epoch: 184  -  cost: 0.9345593 - MSE:  12.088020255198197 -Train Accuracy: 0.45454547\n",
      "epoch: 185  -  cost: 1.0994124 - MSE:  13.561217459571495 -Train Accuracy: 0.58181816\n",
      "epoch: 186  -  cost: 0.92916757 - MSE:  12.113848563836175 -Train Accuracy: 0.45454547\n",
      "epoch: 187  -  cost: 1.0907726 - MSE:  13.55130289631017 -Train Accuracy: 0.58181816\n",
      "epoch: 188  -  cost: 0.9240624 - MSE:  12.141871699209023 -Train Accuracy: 0.45454547\n",
      "epoch: 189  -  cost: 1.082676 - MSE:  13.546095800138914 -Train Accuracy: 0.58181816\n",
      "epoch: 190  -  cost: 0.91924006 - MSE:  12.171044710015181 -Train Accuracy: 0.45454547\n",
      "epoch: 191  -  cost: 1.0751269 - MSE:  13.544704009510456 -Train Accuracy: 0.58181816\n",
      "epoch: 192  -  cost: 0.9146884 - MSE:  12.200459344088918 -Train Accuracy: 0.45454547\n",
      "epoch: 193  -  cost: 1.068109 - MSE:  13.546293801436864 -Train Accuracy: 0.58181816\n",
      "epoch: 194  -  cost: 0.91038877 - MSE:  12.22938403239377 -Train Accuracy: 0.45454547\n",
      "epoch: 195  -  cost: 1.0615939 - MSE:  13.55013792966253 -Train Accuracy: 0.58181816\n",
      "epoch: 196  -  cost: 0.90631974 - MSE:  12.257264005495044 -Train Accuracy: 0.45454547\n",
      "epoch: 197  -  cost: 1.0555418 - MSE:  13.555600890705117 -Train Accuracy: 0.58181816\n",
      "epoch: 198  -  cost: 0.90245825 - MSE:  12.283702169637175 -Train Accuracy: 0.45454547\n",
      "epoch: 199  -  cost: 1.0499092 - MSE:  13.562144994786836 -Train Accuracy: 0.58181816\n",
      "epoch: 200  -  cost: 0.89878196 - MSE:  12.308414716838316 -Train Accuracy: 0.45454547\n",
      "epoch: 201  -  cost: 1.044651 - MSE:  13.569315022698342 -Train Accuracy: 0.58181816\n",
      "epoch: 202  -  cost: 0.89526975 - MSE:  12.33121944410485 -Train Accuracy: 0.45454547\n",
      "epoch: 203  -  cost: 1.0397242 - MSE:  13.576732336264214 -Train Accuracy: 0.58181816\n",
      "epoch: 204  -  cost: 0.8919019 - MSE:  12.35199991992403 -Train Accuracy: 0.45454547\n",
      "epoch: 205  -  cost: 1.0350875 - MSE:  13.584070827856427 -Train Accuracy: 0.58787876\n",
      "epoch: 206  -  cost: 0.88866067 - MSE:  12.37067374895899 -Train Accuracy: 0.45454547\n",
      "epoch: 207  -  cost: 1.0307033 - MSE:  13.591053744079598 -Train Accuracy: 0.5939394\n",
      "epoch: 208  -  cost: 0.88553053 - MSE:  12.387201467651586 -Train Accuracy: 0.45454547\n",
      "epoch: 209  -  cost: 1.0265378 - MSE:  13.597441693253442 -Train Accuracy: 0.5939394\n",
      "epoch: 210  -  cost: 0.8824967 - MSE:  12.401542952756333 -Train Accuracy: 0.45454547\n",
      "epoch: 211  -  cost: 1.022561 - MSE:  13.603016315939472 -Train Accuracy: 0.5939394\n",
      "epoch: 212  -  cost: 0.87954694 - MSE:  12.413666545045263 -Train Accuracy: 0.45454547\n",
      "epoch: 213  -  cost: 1.0187454 - MSE:  13.607583310638146 -Train Accuracy: 0.5939394\n",
      "epoch: 214  -  cost: 0.8766696 - MSE:  12.423540367295372 -Train Accuracy: 0.45454547\n",
      "epoch: 215  -  cost: 1.0150672 - MSE:  13.610955296063903 -Train Accuracy: 0.5939394\n",
      "epoch: 216  -  cost: 0.87385446 - MSE:  12.431121937287779 -Train Accuracy: 0.45454547\n",
      "epoch: 217  -  cost: 1.0115047 - MSE:  13.61295834003998 -Train Accuracy: 0.5939394\n",
      "epoch: 218  -  cost: 0.8710919 - MSE:  12.436361476670543 -Train Accuracy: 0.45454547\n",
      "epoch: 219  -  cost: 1.0080382 - MSE:  13.61341395492235 -Train Accuracy: 0.5939394\n",
      "epoch: 220  -  cost: 0.86837345 - MSE:  12.43919110255994 -Train Accuracy: 0.45454547\n",
      "epoch: 221  -  cost: 1.0046494 - MSE:  13.612150310970971 -Train Accuracy: 0.5939394\n",
      "epoch: 222  -  cost: 0.865691 - MSE:  12.43953567463501 -Train Accuracy: 0.45454547\n",
      "epoch: 223  -  cost: 1.0013224 - MSE:  13.608988578187999 -Train Accuracy: 0.5939394\n",
      "epoch: 224  -  cost: 0.8630372 - MSE:  12.437297673155529 -Train Accuracy: 0.45454547\n",
      "epoch: 225  -  cost: 0.99804246 - MSE:  13.603745255294163 -Train Accuracy: 0.5939394\n",
      "epoch: 226  -  cost: 0.86040473 - MSE:  12.432369223974243 -Train Accuracy: 0.45454547\n",
      "epoch: 227  -  cost: 0.9947952 - MSE:  13.596232306615057 -Train Accuracy: 0.6\n",
      "epoch: 228  -  cost: 0.8577869 - MSE:  12.424617178957233 -Train Accuracy: 0.45454547\n",
      "epoch: 229  -  cost: 0.9915674 - MSE:  13.586246287330434 -Train Accuracy: 0.6\n",
      "epoch: 230  -  cost: 0.85517716 - MSE:  12.413896854299022 -Train Accuracy: 0.45454547\n",
      "epoch: 231  -  cost: 0.9883468 - MSE:  13.573577940993427 -Train Accuracy: 0.6\n",
      "epoch: 232  -  cost: 0.8525691 - MSE:  12.400040371914264 -Train Accuracy: 0.45454547\n",
      "epoch: 233  -  cost: 0.98512083 - MSE:  13.557999277578343 -Train Accuracy: 0.6\n",
      "epoch: 234  -  cost: 0.84995615 - MSE:  12.38285480637152 -Train Accuracy: 0.45454547\n",
      "epoch: 235  -  cost: 0.98187876 - MSE:  13.539275584314007 -Train Accuracy: 0.6\n",
      "epoch: 236  -  cost: 0.8473325 - MSE:  12.362135251711853 -Train Accuracy: 0.45454547\n",
      "epoch: 237  -  cost: 0.9786092 - MSE:  13.517149737170438 -Train Accuracy: 0.6\n",
      "epoch: 238  -  cost: 0.8446915 - MSE:  12.3376459474467 -Train Accuracy: 0.45454547\n",
      "epoch: 239  -  cost: 0.97530055 - MSE:  13.491349847744967 -Train Accuracy: 0.6\n",
      "epoch: 240  -  cost: 0.84202707 - MSE:  12.309131968000026 -Train Accuracy: 0.45454547\n",
      "epoch: 241  -  cost: 0.9719425 - MSE:  13.461588372772237 -Train Accuracy: 0.6\n",
      "epoch: 242  -  cost: 0.83933246 - MSE:  12.276314646358568 -Train Accuracy: 0.45454547\n",
      "epoch: 243  -  cost: 0.96852344 - MSE:  13.427553637563822 -Train Accuracy: 0.6\n",
      "epoch: 244  -  cost: 0.8366017 - MSE:  12.238888981905347 -Train Accuracy: 0.45454547\n",
      "epoch: 245  -  cost: 0.96503276 - MSE:  13.388920110272815 -Train Accuracy: 0.6\n",
      "epoch: 246  -  cost: 0.8338279 - MSE:  12.196528643428424 -Train Accuracy: 0.45454547\n",
      "epoch: 247  -  cost: 0.96145993 - MSE:  13.345341778082073 -Train Accuracy: 0.6\n",
      "epoch: 248  -  cost: 0.8310047 - MSE:  12.14888042342605 -Train Accuracy: 0.45454547\n",
      "epoch: 249  -  cost: 0.95779353 - MSE:  13.296459482665915 -Train Accuracy: 0.6\n",
      "epoch: 250  -  cost: 0.828125 - MSE:  12.095582952597036 -Train Accuracy: 0.45454547\n",
      "epoch: 251  -  cost: 0.9540226 - MSE:  13.24190234157283 -Train Accuracy: 0.6\n",
      "epoch: 252  -  cost: 0.8251831 - MSE:  12.036250276168621 -Train Accuracy: 0.45454547\n",
      "epoch: 253  -  cost: 0.9501371 - MSE:  13.181294566046693 -Train Accuracy: 0.6\n",
      "epoch: 254  -  cost: 0.82217205 - MSE:  11.97049193348321 -Train Accuracy: 0.45454547\n",
      "epoch: 255  -  cost: 0.9461261 - MSE:  13.114266245487789 -Train Accuracy: 0.6\n",
      "epoch: 256  -  cost: 0.8190861 - MSE:  11.897927916950815 -Train Accuracy: 0.45454547\n",
      "epoch: 257  -  cost: 0.94198 - MSE:  13.040463971470396 -Train Accuracy: 0.6\n",
      "epoch: 258  -  cost: 0.81591934 - MSE:  11.818200626695678 -Train Accuracy: 0.45454547\n",
      "epoch: 259  -  cost: 0.9376905 - MSE:  12.959574265335501 -Train Accuracy: 0.6\n",
      "epoch: 260  -  cost: 0.81266785 - MSE:  11.73098682246712 -Train Accuracy: 0.45454547\n",
      "epoch: 261  -  cost: 0.93325096 - MSE:  12.871337101230356 -Train Accuracy: 0.6\n",
      "epoch: 262  -  cost: 0.8093279 - MSE:  11.636038708645827 -Train Accuracy: 0.45454547\n",
      "epoch: 263  -  cost: 0.9286563 - MSE:  12.775578323733523 -Train Accuracy: 0.6\n",
      "epoch: 264  -  cost: 0.8058975 - MSE:  11.53320344201633 -Train Accuracy: 0.45454547\n",
      "epoch: 265  -  cost: 0.9239048 - MSE:  12.672240352702293 -Train Accuracy: 0.6\n",
      "epoch: 266  -  cost: 0.8023769 - MSE:  11.422472763489756 -Train Accuracy: 0.45454547\n",
      "epoch: 267  -  cost: 0.91899884 - MSE:  12.56141422189453 -Train Accuracy: 0.6\n",
      "epoch: 268  -  cost: 0.7987693 - MSE:  11.30400980235658 -Train Accuracy: 0.45454547\n",
      "epoch: 269  -  cost: 0.9139456 - MSE:  12.443371678572058 -Train Accuracy: 0.6060606\n",
      "epoch: 270  -  cost: 0.7950791 - MSE:  11.17819067370248 -Train Accuracy: 0.45454547\n",
      "epoch: 271  -  cost: 0.9087566 - MSE:  12.318588139426922 -Train Accuracy: 0.6060606\n",
      "epoch: 272  -  cost: 0.79131573 - MSE:  11.045634239684333 -Train Accuracy: 0.45454547\n",
      "epoch: 273  -  cost: 0.9034504 - MSE:  12.187767292163025 -Train Accuracy: 0.6121212\n",
      "epoch: 274  -  cost: 0.78749055 - MSE:  10.907220974594587 -Train Accuracy: 0.45454547\n",
      "epoch: 275  -  cost: 0.8980513 - MSE:  12.051831451070177 -Train Accuracy: 0.6121212\n",
      "epoch: 276  -  cost: 0.7836178 - MSE:  10.76407312737031 -Train Accuracy: 0.45454547\n",
      "epoch: 277  -  cost: 0.8925873 - MSE:  11.911898491175956 -Train Accuracy: 0.6181818\n",
      "epoch: 278  -  cost: 0.77971476 - MSE:  10.617537723864382 -Train Accuracy: 0.45454547\n",
      "epoch: 279  -  cost: 0.8870925 - MSE:  11.769249339601885 -Train Accuracy: 0.6181818\n",
      "epoch: 280  -  cost: 0.7757998 - MSE:  10.469118182429153 -Train Accuracy: 0.45454547\n",
      "epoch: 281  -  cost: 0.88160235 - MSE:  11.625253051705094 -Train Accuracy: 0.6181818\n",
      "epoch: 282  -  cost: 0.77189213 - MSE:  10.320396053535346 -Train Accuracy: 0.45454547\n",
      "epoch: 283  -  cost: 0.87615263 - MSE:  11.481297293522184 -Train Accuracy: 0.6242424\n",
      "epoch: 284  -  cost: 0.7680102 - MSE:  10.17294148428023 -Train Accuracy: 0.45454547\n",
      "epoch: 285  -  cost: 0.87077767 - MSE:  11.338707646143042 -Train Accuracy: 0.6242424\n",
      "epoch: 286  -  cost: 0.7641705 - MSE:  10.028211204322169 -Train Accuracy: 0.45454547\n",
      "epoch: 287  -  cost: 0.86550665 - MSE:  11.198676960623143 -Train Accuracy: 0.6242424\n",
      "epoch: 288  -  cost: 0.7603874 - MSE:  9.88747903747409 -Train Accuracy: 0.45454547\n",
      "epoch: 289  -  cost: 0.86036384 - MSE:  11.062212628144085 -Train Accuracy: 0.6242424\n",
      "epoch: 290  -  cost: 0.75667244 - MSE:  9.751776919426979 -Train Accuracy: 0.45454547\n",
      "epoch: 291  -  cost: 0.8553678 - MSE:  10.930107055526808 -Train Accuracy: 0.6242424\n",
      "epoch: 292  -  cost: 0.7530332 - MSE:  9.621871169704693 -Train Accuracy: 0.45454547\n",
      "epoch: 293  -  cost: 0.85052973 - MSE:  10.802930930916324 -Train Accuracy: 0.630303\n",
      "epoch: 294  -  cost: 0.7494744 - MSE:  9.498257856344607 -Train Accuracy: 0.45454547\n",
      "epoch: 295  -  cost: 0.8458549 - MSE:  10.68103661863049 -Train Accuracy: 0.630303\n",
      "epoch: 296  -  cost: 0.7459985 - MSE:  9.381194530519632 -Train Accuracy: 0.45454547\n",
      "epoch: 297  -  cost: 0.84134465 - MSE:  10.564592782600021 -Train Accuracy: 0.6424242\n",
      "epoch: 298  -  cost: 0.74260527 - MSE:  9.27072563508258 -Train Accuracy: 0.45454547\n",
      "epoch: 299  -  cost: 0.8369952 - MSE:  10.453608167268559 -Train Accuracy: 0.6424242\n",
      "epoch: 300  -  cost: 0.73929256 - MSE:  9.166727854734223 -Train Accuracy: 0.45454547\n",
      "epoch: 301  -  cost: 0.83280027 - MSE:  10.347967435476013 -Train Accuracy: 0.6424242\n",
      "epoch: 302  -  cost: 0.73605746 - MSE:  9.06895611476089 -Train Accuracy: 0.45454547\n",
      "epoch: 303  -  cost: 0.8287528 - MSE:  10.247468271973705 -Train Accuracy: 0.6424242\n",
      "epoch: 304  -  cost: 0.7328961 - MSE:  8.977074764088794 -Train Accuracy: 0.46060607\n",
      "epoch: 305  -  cost: 0.8248432 - MSE:  10.151844148425695 -Train Accuracy: 0.6424242\n",
      "epoch: 306  -  cost: 0.72980434 - MSE:  8.890698137648757 -Train Accuracy: 0.46060607\n",
      "epoch: 307  -  cost: 0.82106304 - MSE:  10.060791473822535 -Train Accuracy: 0.6424242\n",
      "epoch: 308  -  cost: 0.7267779 - MSE:  8.809413006819556 -Train Accuracy: 0.46666667\n",
      "epoch: 309  -  cost: 0.81740355 - MSE:  9.973986836459966 -Train Accuracy: 0.6484848\n",
      "epoch: 310  -  cost: 0.72381264 - MSE:  8.732790797914086 -Train Accuracy: 0.47878787\n",
      "epoch: 311  -  cost: 0.8138558 - MSE:  9.891098102604824 -Train Accuracy: 0.6484848\n",
      "epoch: 312  -  cost: 0.72090423 - MSE:  8.66041181040347 -Train Accuracy: 0.4848485\n",
      "epoch: 313  -  cost: 0.81041193 - MSE:  9.811799699412827 -Train Accuracy: 0.6484848\n",
      "epoch: 314  -  cost: 0.71804893 - MSE:  8.591874603945405 -Train Accuracy: 0.4848485\n",
      "epoch: 315  -  cost: 0.8070649 - MSE:  9.735779990734855 -Train Accuracy: 0.6484848\n",
      "epoch: 316  -  cost: 0.71524304 - MSE:  8.526791087175534 -Train Accuracy: 0.4909091\n",
      "epoch: 317  -  cost: 0.80380803 - MSE:  9.662733638722909 -Train Accuracy: 0.6484848\n",
      "epoch: 318  -  cost: 0.7124836 - MSE:  8.464803810810835 -Train Accuracy: 0.4909091\n",
      "epoch: 319  -  cost: 0.8006357 - MSE:  9.59238016051598 -Train Accuracy: 0.6484848\n",
      "epoch: 320  -  cost: 0.70976716 - MSE:  8.405573250190928 -Train Accuracy: 0.4909091\n",
      "epoch: 321  -  cost: 0.79754215 - MSE:  9.524453266164379 -Train Accuracy: 0.6484848\n",
      "epoch: 322  -  cost: 0.70709085 - MSE:  8.348792146050618 -Train Accuracy: 0.4969697\n",
      "epoch: 323  -  cost: 0.7945221 - MSE:  9.45870719610979 -Train Accuracy: 0.6484848\n",
      "epoch: 324  -  cost: 0.7044518 - MSE:  8.294174203187735 -Train Accuracy: 0.5030303\n",
      "epoch: 325  -  cost: 0.79157096 - MSE:  9.394918747011427 -Train Accuracy: 0.6484848\n",
      "epoch: 326  -  cost: 0.7018472 - MSE:  8.241464322973647 -Train Accuracy: 0.4969697\n",
      "epoch: 327  -  cost: 0.78868407 - MSE:  9.332876136990926 -Train Accuracy: 0.6484848\n",
      "epoch: 328  -  cost: 0.69927424 - MSE:  8.190422523517608 -Train Accuracy: 0.4969697\n",
      "epoch: 329  -  cost: 0.78585714 - MSE:  9.272391768931259 -Train Accuracy: 0.6484848\n",
      "epoch: 330  -  cost: 0.69673026 - MSE:  8.140836141798118 -Train Accuracy: 0.4909091\n",
      "epoch: 331  -  cost: 0.78308547 - MSE:  9.213291475093106 -Train Accuracy: 0.6484848\n",
      "epoch: 332  -  cost: 0.69421273 - MSE:  8.092514233934072 -Train Accuracy: 0.4909091\n",
      "epoch: 333  -  cost: 0.7803649 - MSE:  9.15542410743577 -Train Accuracy: 0.6484848\n",
      "epoch: 334  -  cost: 0.69171834 - MSE:  8.045288723023951 -Train Accuracy: 0.4969697\n",
      "epoch: 335  -  cost: 0.7776902 - MSE:  9.09864855920205 -Train Accuracy: 0.6484848\n",
      "epoch: 336  -  cost: 0.6892445 - MSE:  7.999007448332352 -Train Accuracy: 0.5090909\n",
      "epoch: 337  -  cost: 0.7750569 - MSE:  9.042842254741291 -Train Accuracy: 0.6484848\n",
      "epoch: 338  -  cost: 0.6867881 - MSE:  7.953538671540026 -Train Accuracy: 0.5090909\n",
      "epoch: 339  -  cost: 0.7724594 - MSE:  8.987900543911195 -Train Accuracy: 0.6484848\n",
      "epoch: 340  -  cost: 0.68434596 - MSE:  7.9087769523463685 -Train Accuracy: 0.5090909\n",
      "epoch: 341  -  cost: 0.76989263 - MSE:  8.933737634600325 -Train Accuracy: 0.6484848\n",
      "epoch: 342  -  cost: 0.6819148 - MSE:  7.864634427390172 -Train Accuracy: 0.5090909\n",
      "epoch: 343  -  cost: 0.7673505 - MSE:  8.880281549408435 -Train Accuracy: 0.6484848\n",
      "epoch: 344  -  cost: 0.67949146 - MSE:  7.8210430426026525 -Train Accuracy: 0.5090909\n",
      "epoch: 345  -  cost: 0.7648268 - MSE:  8.827477565363413 -Train Accuracy: 0.6484848\n",
      "epoch: 346  -  cost: 0.6770727 - MSE:  7.777957931482989 -Train Accuracy: 0.5151515\n",
      "epoch: 347  -  cost: 0.7623156 - MSE:  8.77529051674453 -Train Accuracy: 0.6545454\n",
      "epoch: 348  -  cost: 0.6746555 - MSE:  7.735353300674368 -Train Accuracy: 0.5212121\n",
      "epoch: 349  -  cost: 0.75981 - MSE:  8.723700628245908 -Train Accuracy: 0.6545454\n",
      "epoch: 350  -  cost: 0.6722361 - MSE:  7.693227120567163 -Train Accuracy: 0.5212121\n",
      "epoch: 351  -  cost: 0.7573028 - MSE:  8.672704932001755 -Train Accuracy: 0.6545454\n",
      "epoch: 352  -  cost: 0.66981155 - MSE:  7.651596036324991 -Train Accuracy: 0.53333336\n",
      "epoch: 353  -  cost: 0.7547877 - MSE:  8.622317122705212 -Train Accuracy: 0.6545454\n",
      "epoch: 354  -  cost: 0.6673792 - MSE:  7.610498294189573 -Train Accuracy: 0.53333336\n",
      "epoch: 355  -  cost: 0.752258 - MSE:  8.572567093694037 -Train Accuracy: 0.6545454\n",
      "epoch: 356  -  cost: 0.6649365 - MSE:  7.56999037728167 -Train Accuracy: 0.53333336\n",
      "epoch: 357  -  cost: 0.7497077 - MSE:  8.52349674320383 -Train Accuracy: 0.6545454\n",
      "epoch: 358  -  cost: 0.6624815 - MSE:  7.530141626940517 -Train Accuracy: 0.53333336\n",
      "epoch: 359  -  cost: 0.7471319 - MSE:  8.475166535651372 -Train Accuracy: 0.6545454\n",
      "epoch: 360  -  cost: 0.6600128 - MSE:  7.491040120018203 -Train Accuracy: 0.53333336\n",
      "epoch: 361  -  cost: 0.7445256 - MSE:  8.427642740824941 -Train Accuracy: 0.6545454\n",
      "epoch: 362  -  cost: 0.6575294 - MSE:  7.4527821101693315 -Train Accuracy: 0.53939396\n",
      "epoch: 363  -  cost: 0.74188566 - MSE:  8.381001001216472 -Train Accuracy: 0.6606061\n",
      "epoch: 364  -  cost: 0.65503126 - MSE:  7.415470853713278 -Train Accuracy: 0.55151516\n",
      "epoch: 365  -  cost: 0.73921007 - MSE:  8.335322949586322 -Train Accuracy: 0.6606061\n",
      "epoch: 366  -  cost: 0.652519 - MSE:  7.379210603791002 -Train Accuracy: 0.55151516\n",
      "epoch: 367  -  cost: 0.73649806 - MSE:  8.290692754335566 -Train Accuracy: 0.6606061\n",
      "epoch: 368  -  cost: 0.649994 - MSE:  7.344107754504795 -Train Accuracy: 0.55757576\n",
      "epoch: 369  -  cost: 0.7337505 - MSE:  8.247194658120813 -Train Accuracy: 0.6606061\n",
      "epoch: 370  -  cost: 0.647458 - MSE:  7.310263022453829 -Train Accuracy: 0.57575756\n",
      "epoch: 371  -  cost: 0.7309695 - MSE:  8.204912872725851 -Train Accuracy: 0.6666667\n",
      "epoch: 372  -  cost: 0.6449141 - MSE:  7.2777710667348 -Train Accuracy: 0.57575756\n",
      "epoch: 373  -  cost: 0.7281589 - MSE:  8.163925015893266 -Train Accuracy: 0.6666667\n",
      "epoch: 374  -  cost: 0.6423652 - MSE:  7.24671378779136 -Train Accuracy: 0.58787876\n",
      "epoch: 375  -  cost: 0.7253228 - MSE:  8.124301381735709 -Train Accuracy: 0.6666667\n",
      "epoch: 376  -  cost: 0.63981456 - MSE:  7.217163192139491 -Train Accuracy: 0.58787876\n",
      "epoch: 377  -  cost: 0.7224664 - MSE:  8.086101090303002 -Train Accuracy: 0.6666667\n",
      "epoch: 378  -  cost: 0.637266 - MSE:  7.189175277973474 -Train Accuracy: 0.58787876\n",
      "epoch: 379  -  cost: 0.71959555 - MSE:  8.049374361129404 -Train Accuracy: 0.6666667\n",
      "epoch: 380  -  cost: 0.63472307 - MSE:  7.162796675182658 -Train Accuracy: 0.6\n",
      "epoch: 381  -  cost: 0.71671635 - MSE:  8.014162196513169 -Train Accuracy: 0.6666667\n",
      "epoch: 382  -  cost: 0.63218933 - MSE:  7.138052713744003 -Train Accuracy: 0.6\n",
      "epoch: 383  -  cost: 0.7138345 - MSE:  7.980485005940461 -Train Accuracy: 0.6666667\n",
      "epoch: 384  -  cost: 0.62966824 - MSE:  7.114958886197785 -Train Accuracy: 0.6\n",
      "epoch: 385  -  cost: 0.7109557 - MSE:  7.948357684898239 -Train Accuracy: 0.6666667\n",
      "epoch: 386  -  cost: 0.6271623 - MSE:  7.093515373206823 -Train Accuracy: 0.6\n",
      "epoch: 387  -  cost: 0.70808464 - MSE:  7.9177794757543625 -Train Accuracy: 0.6666667\n",
      "epoch: 388  -  cost: 0.62467426 - MSE:  7.073709879715192 -Train Accuracy: 0.6060606\n",
      "epoch: 389  -  cost: 0.70522606 - MSE:  7.888740959949703 -Train Accuracy: 0.6666667\n",
      "epoch: 390  -  cost: 0.62220633 - MSE:  7.05552113194932 -Train Accuracy: 0.6060606\n",
      "epoch: 391  -  cost: 0.70238394 - MSE:  7.861219077406107 -Train Accuracy: 0.6666667\n",
      "epoch: 392  -  cost: 0.6197599 - MSE:  7.0389152249797595 -Train Accuracy: 0.6060606\n",
      "epoch: 393  -  cost: 0.6995609 - MSE:  7.835184541155051 -Train Accuracy: 0.6666667\n",
      "epoch: 394  -  cost: 0.6173363 - MSE:  7.023853100889691 -Train Accuracy: 0.6121212\n",
      "epoch: 395  -  cost: 0.6967595 - MSE:  7.810599387798685 -Train Accuracy: 0.6666667\n",
      "epoch: 396  -  cost: 0.6149361 - MSE:  7.010289133286684 -Train Accuracy: 0.6181818\n",
      "epoch: 397  -  cost: 0.6939815 - MSE:  7.7874199229813605 -Train Accuracy: 0.6666667\n",
      "epoch: 398  -  cost: 0.61255985 - MSE:  6.998171068490178 -Train Accuracy: 0.6181818\n",
      "epoch: 399  -  cost: 0.69122773 - MSE:  7.7655984054743 -Train Accuracy: 0.6666667\n",
      "epoch: 400  -  cost: 0.61020756 - MSE:  6.987444547742371 -Train Accuracy: 0.6242424\n",
      "epoch: 401  -  cost: 0.6884992 - MSE:  7.745085699215737 -Train Accuracy: 0.6666667\n",
      "epoch: 402  -  cost: 0.60787874 - MSE:  6.978054762713403 -Train Accuracy: 0.630303\n",
      "epoch: 403  -  cost: 0.68579525 - MSE:  7.7258259109441765 -Train Accuracy: 0.6727273\n",
      "epoch: 404  -  cost: 0.60557294 - MSE:  6.969945601350695 -Train Accuracy: 0.6363636\n",
      "epoch: 405  -  cost: 0.6831159 - MSE:  7.707766157696413 -Train Accuracy: 0.6727273\n",
      "epoch: 406  -  cost: 0.60328984 - MSE:  6.963059832571606 -Train Accuracy: 0.6424242\n",
      "epoch: 407  -  cost: 0.6804609 - MSE:  7.690857289920067 -Train Accuracy: 0.6727273\n",
      "epoch: 408  -  cost: 0.6010283 - MSE:  6.957342955031358 -Train Accuracy: 0.6424242\n",
      "epoch: 409  -  cost: 0.67782915 - MSE:  7.675045374242121 -Train Accuracy: 0.6727273\n",
      "epoch: 410  -  cost: 0.5987874 - MSE:  6.952740529935733 -Train Accuracy: 0.6424242\n",
      "epoch: 411  -  cost: 0.6752196 - MSE:  7.660280162698621 -Train Accuracy: 0.6787879\n",
      "epoch: 412  -  cost: 0.5965664 - MSE:  6.949199944831291 -Train Accuracy: 0.6484848\n",
      "epoch: 413  -  cost: 0.67263156 - MSE:  7.646515366616646 -Train Accuracy: 0.6787879\n",
      "epoch: 414  -  cost: 0.59436405 - MSE:  6.946677060946435 -Train Accuracy: 0.6484848\n",
      "epoch: 415  -  cost: 0.6700638 - MSE:  7.633705639343388 -Train Accuracy: 0.6787879\n",
      "epoch: 416  -  cost: 0.5921793 - MSE:  6.945121244771279 -Train Accuracy: 0.6484848\n",
      "epoch: 417  -  cost: 0.6675149 - MSE:  7.621807192559652 -Train Accuracy: 0.6787879\n",
      "epoch: 418  -  cost: 0.5900115 - MSE:  6.944491638593114 -Train Accuracy: 0.6484848\n",
      "epoch: 419  -  cost: 0.6649839 - MSE:  7.610782478789655 -Train Accuracy: 0.6787879\n",
      "epoch: 420  -  cost: 0.5878593 - MSE:  6.944747533088481 -Train Accuracy: 0.6484848\n",
      "epoch: 421  -  cost: 0.66246986 - MSE:  7.600595175822015 -Train Accuracy: 0.6787879\n",
      "epoch: 422  -  cost: 0.5857219 - MSE:  6.945853324252762 -Train Accuracy: 0.6424242\n",
      "epoch: 423  -  cost: 0.6599714 - MSE:  7.591209563600008 -Train Accuracy: 0.6787879\n",
      "epoch: 424  -  cost: 0.583598 - MSE:  6.947771194467973 -Train Accuracy: 0.6484848\n",
      "epoch: 425  -  cost: 0.6574873 - MSE:  7.582594328045259 -Train Accuracy: 0.6848485\n",
      "epoch: 426  -  cost: 0.58148694 - MSE:  6.950470466166329 -Train Accuracy: 0.6484848\n",
      "epoch: 427  -  cost: 0.6550162 - MSE:  7.574720886389282 -Train Accuracy: 0.6848485\n",
      "epoch: 428  -  cost: 0.57938784 - MSE:  6.953922528660856 -Train Accuracy: 0.6484848\n",
      "epoch: 429  -  cost: 0.65255827 - MSE:  7.567566903448731 -Train Accuracy: 0.6909091\n",
      "epoch: 430  -  cost: 0.57729983 - MSE:  6.958101180220413 -Train Accuracy: 0.6484848\n",
      "epoch: 431  -  cost: 0.6501115 - MSE:  7.561104171391186 -Train Accuracy: 0.6969697\n",
      "epoch: 432  -  cost: 0.57522184 - MSE:  6.962982284611041 -Train Accuracy: 0.6484848\n",
      "epoch: 433  -  cost: 0.6476753 - MSE:  7.5553140203769695 -Train Accuracy: 0.6969697\n",
      "epoch: 434  -  cost: 0.5731536 - MSE:  6.96854147009293 -Train Accuracy: 0.6484848\n",
      "epoch: 435  -  cost: 0.64524907 - MSE:  7.550176729511647 -Train Accuracy: 0.6969697\n",
      "epoch: 436  -  cost: 0.571094 - MSE:  6.974761462470745 -Train Accuracy: 0.6545454\n",
      "epoch: 437  -  cost: 0.64283186 - MSE:  7.54567559274187 -Train Accuracy: 0.6969697\n",
      "epoch: 438  -  cost: 0.56904256 - MSE:  6.981624005212571 -Train Accuracy: 0.6606061\n",
      "epoch: 439  -  cost: 0.64042294 - MSE:  7.541793420850091 -Train Accuracy: 0.6969697\n",
      "epoch: 440  -  cost: 0.5669985 - MSE:  6.989109761572292 -Train Accuracy: 0.6606061\n",
      "epoch: 441  -  cost: 0.6380218 - MSE:  7.538518571708768 -Train Accuracy: 0.7030303\n",
      "epoch: 442  -  cost: 0.5649615 - MSE:  6.9972055862648945 -Train Accuracy: 0.6606061\n",
      "epoch: 443  -  cost: 0.6356278 - MSE:  7.535836700872629 -Train Accuracy: 0.7030303\n",
      "epoch: 444  -  cost: 0.56293046 - MSE:  7.005896989681449 -Train Accuracy: 0.6606061\n",
      "epoch: 445  -  cost: 0.6332401 - MSE:  7.533736946152429 -Train Accuracy: 0.7030303\n",
      "epoch: 446  -  cost: 0.5609054 - MSE:  7.01517277782977 -Train Accuracy: 0.6666667\n",
      "epoch: 447  -  cost: 0.6308589 - MSE:  7.532210854165622 -Train Accuracy: 0.7090909\n",
      "epoch: 448  -  cost: 0.55888563 - MSE:  7.025018455690201 -Train Accuracy: 0.6666667\n",
      "epoch: 449  -  cost: 0.62848306 - MSE:  7.531247400682373 -Train Accuracy: 0.7090909\n",
      "epoch: 450  -  cost: 0.5568707 - MSE:  7.035425058066223 -Train Accuracy: 0.6727273\n",
      "epoch: 451  -  cost: 0.6261126 - MSE:  7.530837731035065 -Train Accuracy: 0.7090909\n",
      "epoch: 452  -  cost: 0.5548602 - MSE:  7.046381092050031 -Train Accuracy: 0.6787879\n",
      "epoch: 453  -  cost: 0.62374705 - MSE:  7.530976940437253 -Train Accuracy: 0.7090909\n",
      "epoch: 454  -  cost: 0.55285376 - MSE:  7.057880242370748 -Train Accuracy: 0.6787879\n",
      "epoch: 455  -  cost: 0.62138605 - MSE:  7.5316567730186765 -Train Accuracy: 0.7090909\n",
      "epoch: 456  -  cost: 0.5508509 - MSE:  7.0699092812810544 -Train Accuracy: 0.6787879\n",
      "epoch: 457  -  cost: 0.61902934 - MSE:  7.532870497844781 -Train Accuracy: 0.7090909\n",
      "epoch: 458  -  cost: 0.54885155 - MSE:  7.082461575868936 -Train Accuracy: 0.6848485\n",
      "epoch: 459  -  cost: 0.61667705 - MSE:  7.534613475209973 -Train Accuracy: 0.7090909\n",
      "epoch: 460  -  cost: 0.54685533 - MSE:  7.095527231902871 -Train Accuracy: 0.6848485\n",
      "epoch: 461  -  cost: 0.6143286 - MSE:  7.536878711154039 -Train Accuracy: 0.7090909\n",
      "epoch: 462  -  cost: 0.544862 - MSE:  7.109098930395388 -Train Accuracy: 0.6848485\n",
      "epoch: 463  -  cost: 0.61198425 - MSE:  7.539661406872956 -Train Accuracy: 0.7090909\n",
      "epoch: 464  -  cost: 0.54287124 - MSE:  7.123166344356478 -Train Accuracy: 0.6848485\n",
      "epoch: 465  -  cost: 0.6096438 - MSE:  7.542957014145233 -Train Accuracy: 0.7090909\n",
      "epoch: 466  -  cost: 0.5408831 - MSE:  7.1377228339119805 -Train Accuracy: 0.6909091\n",
      "epoch: 467  -  cost: 0.6073074 - MSE:  7.546759646115953 -Train Accuracy: 0.7090909\n",
      "epoch: 468  -  cost: 0.53889734 - MSE:  7.152757863307647 -Train Accuracy: 0.7151515\n",
      "epoch: 469  -  cost: 0.6049747 - MSE:  7.551065545454381 -Train Accuracy: 0.7090909\n",
      "epoch: 470  -  cost: 0.5369138 - MSE:  7.168261962492928 -Train Accuracy: 0.7151515\n",
      "epoch: 471  -  cost: 0.6026462 - MSE:  7.555866792955635 -Train Accuracy: 0.7151515\n",
      "epoch: 472  -  cost: 0.53493243 - MSE:  7.184226448054468 -Train Accuracy: 0.72727275\n",
      "epoch: 473  -  cost: 0.60032177 - MSE:  7.561159199560347 -Train Accuracy: 0.7151515\n",
      "epoch: 474  -  cost: 0.53295314 - MSE:  7.200639662991599 -Train Accuracy: 0.73333335\n",
      "epoch: 475  -  cost: 0.5980017 - MSE:  7.5669365423600485 -Train Accuracy: 0.7151515\n",
      "epoch: 476  -  cost: 0.53097606 - MSE:  7.217490885208812 -Train Accuracy: 0.73333335\n",
      "epoch: 477  -  cost: 0.59568614 - MSE:  7.573193540339603 -Train Accuracy: 0.7151515\n",
      "epoch: 478  -  cost: 0.5290011 - MSE:  7.234771087809052 -Train Accuracy: 0.73333335\n",
      "epoch: 479  -  cost: 0.5933754 - MSE:  7.579922675465333 -Train Accuracy: 0.7151515\n",
      "epoch: 480  -  cost: 0.5270282 - MSE:  7.252463581249946 -Train Accuracy: 0.73333335\n",
      "epoch: 481  -  cost: 0.59106946 - MSE:  7.587117844816954 -Train Accuracy: 0.7151515\n",
      "epoch: 482  -  cost: 0.52505773 - MSE:  7.2705588716900476 -Train Accuracy: 0.73333335\n",
      "epoch: 483  -  cost: 0.58876896 - MSE:  7.5947707515733995 -Train Accuracy: 0.7151515\n",
      "epoch: 484  -  cost: 0.5230894 - MSE:  7.28904303502662 -Train Accuracy: 0.73333335\n",
      "epoch: 485  -  cost: 0.58647424 - MSE:  7.602873953397368 -Train Accuracy: 0.7151515\n",
      "epoch: 486  -  cost: 0.52112377 - MSE:  7.307899974682757 -Train Accuracy: 0.73333335\n",
      "epoch: 487  -  cost: 0.5841857 - MSE:  7.611419783472172 -Train Accuracy: 0.7151515\n",
      "epoch: 488  -  cost: 0.5191607 - MSE:  7.327114374846515 -Train Accuracy: 0.74545455\n",
      "epoch: 489  -  cost: 0.5819034 - MSE:  7.620398196828709 -Train Accuracy: 0.72121215\n",
      "epoch: 490  -  cost: 0.51720047 - MSE:  7.346673141475882 -Train Accuracy: 0.74545455\n",
      "epoch: 491  -  cost: 0.57962817 - MSE:  7.629799859498435 -Train Accuracy: 0.72121215\n",
      "epoch: 492  -  cost: 0.51524335 - MSE:  7.366554667284558 -Train Accuracy: 0.74545455\n",
      "epoch: 493  -  cost: 0.57736033 - MSE:  7.639614912539436 -Train Accuracy: 0.72121215\n",
      "epoch: 494  -  cost: 0.5132896 - MSE:  7.386746399321499 -Train Accuracy: 0.74545455\n",
      "epoch: 495  -  cost: 0.5751008 - MSE:  7.649834878529651 -Train Accuracy: 0.72727275\n",
      "epoch: 496  -  cost: 0.5113395 - MSE:  7.407229050790733 -Train Accuracy: 0.74545455\n",
      "epoch: 497  -  cost: 0.57284963 - MSE:  7.660446453917945 -Train Accuracy: 0.72727275\n",
      "epoch: 498  -  cost: 0.5093932 - MSE:  7.427982233666386 -Train Accuracy: 0.75151515\n",
      "epoch: 499  -  cost: 0.57060736 - MSE:  7.671438215860648 -Train Accuracy: 0.72727275\n",
      "epoch: 500  -  cost: 0.5074511 - MSE:  7.448987778499362 -Train Accuracy: 0.74545455\n",
      "epoch: 501  -  cost: 0.5683746 - MSE:  7.682798985852528 -Train Accuracy: 0.72727275\n",
      "epoch: 502  -  cost: 0.50551355 - MSE:  7.470227637867367 -Train Accuracy: 0.74545455\n",
      "epoch: 503  -  cost: 0.56615216 - MSE:  7.694518251629172 -Train Accuracy: 0.72727275\n",
      "epoch: 504  -  cost: 0.50358075 - MSE:  7.4916773333896565 -Train Accuracy: 0.75151515\n",
      "epoch: 505  -  cost: 0.5639402 - MSE:  7.706580095872222 -Train Accuracy: 0.72727275\n",
      "epoch: 506  -  cost: 0.50165343 - MSE:  7.513322426290753 -Train Accuracy: 0.75151515\n",
      "epoch: 507  -  cost: 0.56174 - MSE:  7.718973314029497 -Train Accuracy: 0.72727275\n",
      "epoch: 508  -  cost: 0.4997317 - MSE:  7.535137396950633 -Train Accuracy: 0.75757575\n",
      "epoch: 509  -  cost: 0.55955166 - MSE:  7.731685447403828 -Train Accuracy: 0.73333335\n",
      "epoch: 510  -  cost: 0.49781582 - MSE:  7.557103811840756 -Train Accuracy: 0.75757575\n",
      "epoch: 511  -  cost: 0.5573758 - MSE:  7.744703095230012 -Train Accuracy: 0.73333335\n",
      "epoch: 512  -  cost: 0.49590647 - MSE:  7.57920198731117 -Train Accuracy: 0.75757575\n",
      "epoch: 513  -  cost: 0.55521315 - MSE:  7.758011049631337 -Train Accuracy: 0.73939395\n",
      "epoch: 514  -  cost: 0.49400356 - MSE:  7.601408583972947 -Train Accuracy: 0.76969695\n",
      "epoch: 515  -  cost: 0.55306417 - MSE:  7.771594353631061 -Train Accuracy: 0.73939395\n",
      "epoch: 516  -  cost: 0.49210805 - MSE:  7.62370484891287 -Train Accuracy: 0.76969695\n",
      "epoch: 517  -  cost: 0.55092955 - MSE:  7.785441965429051 -Train Accuracy: 0.73939395\n",
      "epoch: 518  -  cost: 0.49022004 - MSE:  7.646069896028065 -Train Accuracy: 0.76969695\n",
      "epoch: 519  -  cost: 0.5488097 - MSE:  7.799540069904267 -Train Accuracy: 0.73939395\n",
      "epoch: 520  -  cost: 0.48834002 - MSE:  7.668486497619194 -Train Accuracy: 0.76969695\n",
      "epoch: 521  -  cost: 0.54670525 - MSE:  7.813874488386145 -Train Accuracy: 0.73939395\n",
      "epoch: 522  -  cost: 0.48646808 - MSE:  7.690932441767524 -Train Accuracy: 0.76969695\n",
      "epoch: 523  -  cost: 0.5446167 - MSE:  7.828431683772015 -Train Accuracy: 0.73939395\n",
      "epoch: 524  -  cost: 0.48460498 - MSE:  7.713390915904931 -Train Accuracy: 0.76969695\n",
      "epoch: 525  -  cost: 0.54254466 - MSE:  7.843197027835774 -Train Accuracy: 0.73939395\n",
      "epoch: 526  -  cost: 0.48275086 - MSE:  7.73584133689697 -Train Accuracy: 0.7878788\n",
      "epoch: 527  -  cost: 0.5404895 - MSE:  7.858159303433096 -Train Accuracy: 0.73939395\n",
      "epoch: 528  -  cost: 0.48090607 - MSE:  7.7582699264083965 -Train Accuracy: 0.7878788\n",
      "epoch: 529  -  cost: 0.53845143 - MSE:  7.873306189917876 -Train Accuracy: 0.73939395\n",
      "epoch: 530  -  cost: 0.47907075 - MSE:  7.78065863250996 -Train Accuracy: 0.7878788\n",
      "epoch: 531  -  cost: 0.53643084 - MSE:  7.888622427692886 -Train Accuracy: 0.73939395\n",
      "epoch: 532  -  cost: 0.47724545 - MSE:  7.8029927167000945 -Train Accuracy: 0.7878788\n",
      "epoch: 533  -  cost: 0.5344285 - MSE:  7.9040976443347475 -Train Accuracy: 0.74545455\n",
      "epoch: 534  -  cost: 0.47543058 - MSE:  7.825255584472346 -Train Accuracy: 0.7878788\n",
      "epoch: 535  -  cost: 0.53244483 - MSE:  7.91972207901206 -Train Accuracy: 0.74545455\n",
      "epoch: 536  -  cost: 0.4736262 - MSE:  7.847436401065985 -Train Accuracy: 0.7878788\n",
      "epoch: 537  -  cost: 0.53047955 - MSE:  7.935481927069101 -Train Accuracy: 0.74545455\n",
      "epoch: 538  -  cost: 0.4718326 - MSE:  7.869518130039942 -Train Accuracy: 0.7878788\n",
      "epoch: 539  -  cost: 0.5285331 - MSE:  7.95136648492246 -Train Accuracy: 0.74545455\n",
      "epoch: 540  -  cost: 0.47004983 - MSE:  7.891492245440609 -Train Accuracy: 0.7939394\n",
      "epoch: 541  -  cost: 0.5266057 - MSE:  7.96736603706605 -Train Accuracy: 0.74545455\n",
      "epoch: 542  -  cost: 0.46827826 - MSE:  7.913344652822275 -Train Accuracy: 0.7939394\n",
      "epoch: 543  -  cost: 0.52469784 - MSE:  7.983469439010461 -Train Accuracy: 0.74545455\n",
      "epoch: 544  -  cost: 0.46651825 - MSE:  7.935068174974219 -Train Accuracy: 0.8\n",
      "epoch: 545  -  cost: 0.52280945 - MSE:  7.999670983636257 -Train Accuracy: 0.74545455\n",
      "epoch: 546  -  cost: 0.46476966 - MSE:  7.956653753237781 -Train Accuracy: 0.8\n",
      "epoch: 547  -  cost: 0.52094066 - MSE:  8.015959836987902 -Train Accuracy: 0.75151515\n",
      "epoch: 548  -  cost: 0.46303275 - MSE:  7.978091121983528 -Train Accuracy: 0.8\n",
      "epoch: 549  -  cost: 0.5190914 - MSE:  8.032327571983298 -Train Accuracy: 0.75151515\n",
      "epoch: 550  -  cost: 0.46130782 - MSE:  7.999374457971818 -Train Accuracy: 0.8\n",
      "epoch: 551  -  cost: 0.5172621 - MSE:  8.048767606476623 -Train Accuracy: 0.75151515\n",
      "epoch: 552  -  cost: 0.45959464 - MSE:  8.020497053651864 -Train Accuracy: 0.8\n",
      "epoch: 553  -  cost: 0.51545244 - MSE:  8.065272491216566 -Train Accuracy: 0.75151515\n",
      "epoch: 554  -  cost: 0.4578933 - MSE:  8.041450842586888 -Train Accuracy: 0.8\n",
      "epoch: 555  -  cost: 0.51366216 - MSE:  8.081833812549327 -Train Accuracy: 0.75151515\n",
      "epoch: 556  -  cost: 0.45620403 - MSE:  8.062234134787072 -Train Accuracy: 0.8\n",
      "epoch: 557  -  cost: 0.5118917 - MSE:  8.098447703004146 -Train Accuracy: 0.75151515\n",
      "epoch: 558  -  cost: 0.45452696 - MSE:  8.082841051679761 -Train Accuracy: 0.8060606\n",
      "epoch: 559  -  cost: 0.510141 - MSE:  8.11510800524443 -Train Accuracy: 0.75151515\n",
      "epoch: 560  -  cost: 0.45286182 - MSE:  8.103267543141873 -Train Accuracy: 0.8060606\n",
      "epoch: 561  -  cost: 0.5084095 - MSE:  8.131809100623437 -Train Accuracy: 0.75151515\n",
      "epoch: 562  -  cost: 0.45120877 - MSE:  8.123512291021582 -Train Accuracy: 0.8060606\n",
      "epoch: 563  -  cost: 0.5066975 - MSE:  8.148546139212932 -Train Accuracy: 0.75151515\n",
      "epoch: 564  -  cost: 0.4495678 - MSE:  8.1435699358177 -Train Accuracy: 0.8060606\n",
      "epoch: 565  -  cost: 0.5050047 - MSE:  8.165317173290402 -Train Accuracy: 0.75151515\n",
      "epoch: 566  -  cost: 0.44793892 - MSE:  8.163441532758046 -Train Accuracy: 0.8060606\n",
      "epoch: 567  -  cost: 0.503331 - MSE:  8.182114316960693 -Train Accuracy: 0.75151515\n",
      "epoch: 568  -  cost: 0.44632193 - MSE:  8.1831234893656 -Train Accuracy: 0.8060606\n",
      "epoch: 569  -  cost: 0.5016761 - MSE:  8.19893921649119 -Train Accuracy: 0.75151515\n",
      "epoch: 570  -  cost: 0.44471675 - MSE:  8.202615580637861 -Train Accuracy: 0.8060606\n",
      "epoch: 571  -  cost: 0.5000398 - MSE:  8.21578597441738 -Train Accuracy: 0.75151515\n",
      "epoch: 572  -  cost: 0.44312343 - MSE:  8.22191857409613 -Train Accuracy: 0.8060606\n",
      "epoch: 573  -  cost: 0.49842215 - MSE:  8.23265178708733 -Train Accuracy: 0.75151515\n",
      "epoch: 574  -  cost: 0.4415421 - MSE:  8.24102992385818 -Train Accuracy: 0.8060606\n",
      "epoch: 575  -  cost: 0.4968228 - MSE:  8.249536779960588 -Train Accuracy: 0.75151515\n",
      "epoch: 576  -  cost: 0.4399722 - MSE:  8.259948592882086 -Train Accuracy: 0.8060606\n",
      "epoch: 577  -  cost: 0.49524137 - MSE:  8.266434639886418 -Train Accuracy: 0.75151515\n",
      "epoch: 578  -  cost: 0.438414 - MSE:  8.27867602093502 -Train Accuracy: 0.8060606\n",
      "epoch: 579  -  cost: 0.49367818 - MSE:  8.283348748583975 -Train Accuracy: 0.75151515\n",
      "epoch: 580  -  cost: 0.43686745 - MSE:  8.297211391018015 -Train Accuracy: 0.8121212\n",
      "epoch: 581  -  cost: 0.49213275 - MSE:  8.300273323229188 -Train Accuracy: 0.75151515\n",
      "epoch: 582  -  cost: 0.43533224 - MSE:  8.315554173122129 -Train Accuracy: 0.8121212\n",
      "epoch: 583  -  cost: 0.49060476 - MSE:  8.317210362690854 -Train Accuracy: 0.75151515\n",
      "epoch: 584  -  cost: 0.43380824 - MSE:  8.333704972013358 -Train Accuracy: 0.8121212\n",
      "epoch: 585  -  cost: 0.48909393 - MSE:  8.334156360543778 -Train Accuracy: 0.75151515\n",
      "epoch: 586  -  cost: 0.43229547 - MSE:  8.351665043192865 -Train Accuracy: 0.8121212\n",
      "epoch: 587  -  cost: 0.48760024 - MSE:  8.351112590217475 -Train Accuracy: 0.75151515\n",
      "epoch: 588  -  cost: 0.43079367 - MSE:  8.369431435528142 -Train Accuracy: 0.8121212\n",
      "epoch: 589  -  cost: 0.48612374 - MSE:  8.368074778968943 -Train Accuracy: 0.75151515\n",
      "epoch: 590  -  cost: 0.42930308 - MSE:  8.387003908769477 -Train Accuracy: 0.8181818\n",
      "epoch: 591  -  cost: 0.48466423 - MSE:  8.385043324863389 -Train Accuracy: 0.75151515\n",
      "epoch: 592  -  cost: 0.42782333 - MSE:  8.404382126378689 -Train Accuracy: 0.8181818\n",
      "epoch: 593  -  cost: 0.48322147 - MSE:  8.402016884726049 -Train Accuracy: 0.75151515\n",
      "epoch: 594  -  cost: 0.42635453 - MSE:  8.421566114229298 -Train Accuracy: 0.8181818\n",
      "epoch: 595  -  cost: 0.48179558 - MSE:  8.418995437583336 -Train Accuracy: 0.75151515\n",
      "epoch: 596  -  cost: 0.42489642 - MSE:  8.438551887217733 -Train Accuracy: 0.8242424\n",
      "epoch: 597  -  cost: 0.48038626 - MSE:  8.435974967102487 -Train Accuracy: 0.75151515\n",
      "epoch: 598  -  cost: 0.4234492 - MSE:  8.455336511425125 -Train Accuracy: 0.8242424\n",
      "epoch: 599  -  cost: 0.47899416 - MSE:  8.452951428478919 -Train Accuracy: 0.75151515\n",
      "epoch: 600  -  cost: 0.4220129 - MSE:  8.471917835996965 -Train Accuracy: 0.830303\n",
      "epoch: 601  -  cost: 0.47761872 - MSE:  8.469924858802935 -Train Accuracy: 0.75151515\n",
      "epoch: 602  -  cost: 0.42058712 - MSE:  8.488291989229337 -Train Accuracy: 0.830303\n",
      "epoch: 603  -  cost: 0.47626007 - MSE:  8.486888326531481 -Train Accuracy: 0.75151515\n",
      "epoch: 604  -  cost: 0.41917208 - MSE:  8.504455360043366 -Train Accuracy: 0.830303\n",
      "epoch: 605  -  cost: 0.4749185 - MSE:  8.503840387840544 -Train Accuracy: 0.75151515\n",
      "epoch: 606  -  cost: 0.41776803 - MSE:  8.520401401139587 -Train Accuracy: 0.830303\n",
      "epoch: 607  -  cost: 0.47359467 - MSE:  8.520772889166514 -Train Accuracy: 0.75151515\n",
      "epoch: 608  -  cost: 0.41637495 - MSE:  8.536125220159652 -Train Accuracy: 0.830303\n",
      "epoch: 609  -  cost: 0.47228804 - MSE:  8.537681680730975 -Train Accuracy: 0.75757575\n",
      "epoch: 610  -  cost: 0.41499263 - MSE:  8.55161811331048 -Train Accuracy: 0.830303\n",
      "epoch: 611  -  cost: 0.47099897 - MSE:  8.55455280312991 -Train Accuracy: 0.76363635\n",
      "epoch: 612  -  cost: 0.41362157 - MSE:  8.566873013952163 -Train Accuracy: 0.830303\n",
      "epoch: 613  -  cost: 0.46972838 - MSE:  8.571381575577092 -Train Accuracy: 0.76363635\n",
      "epoch: 614  -  cost: 0.4122617 - MSE:  8.58188043078411 -Train Accuracy: 0.830303\n",
      "epoch: 615  -  cost: 0.4684757 - MSE:  8.58815544053709 -Train Accuracy: 0.76363635\n",
      "epoch: 616  -  cost: 0.41091314 - MSE:  8.596633118003325 -Train Accuracy: 0.830303\n",
      "epoch: 617  -  cost: 0.46724173 - MSE:  8.604861290560434 -Train Accuracy: 0.76363635\n",
      "epoch: 618  -  cost: 0.40957615 - MSE:  8.611118559629983 -Train Accuracy: 0.830303\n",
      "epoch: 619  -  cost: 0.46602654 - MSE:  8.62148412068901 -Train Accuracy: 0.76363635\n",
      "epoch: 620  -  cost: 0.40825084 - MSE:  8.625324990979244 -Train Accuracy: 0.830303\n",
      "epoch: 621  -  cost: 0.46483034 - MSE:  8.638007237531909 -Train Accuracy: 0.76363635\n",
      "epoch: 622  -  cost: 0.40693748 - MSE:  8.639244689503947 -Train Accuracy: 0.8363636\n",
      "epoch: 623  -  cost: 0.46365365 - MSE:  8.654416678172604 -Train Accuracy: 0.76363635\n",
      "epoch: 624  -  cost: 0.40563613 - MSE:  8.652867128113249 -Train Accuracy: 0.8363636\n",
      "epoch: 625  -  cost: 0.46249634 - MSE:  8.670691076376045 -Train Accuracy: 0.76363635\n",
      "epoch: 626  -  cost: 0.40434703 - MSE:  8.666177675437188 -Train Accuracy: 0.8363636\n",
      "epoch: 627  -  cost: 0.46135828 - MSE:  8.68680638263692 -Train Accuracy: 0.76363635\n",
      "epoch: 628  -  cost: 0.40307006 - MSE:  8.679167390542593 -Train Accuracy: 0.8363636\n",
      "epoch: 629  -  cost: 0.46023914 - MSE:  8.702743826368728 -Train Accuracy: 0.76363635\n",
      "epoch: 630  -  cost: 0.40180534 - MSE:  8.691826101256408 -Train Accuracy: 0.8363636\n",
      "epoch: 631  -  cost: 0.4591388 - MSE:  8.718479905511971 -Train Accuracy: 0.76363635\n",
      "epoch: 632  -  cost: 0.4005527 - MSE:  8.704144867185793 -Train Accuracy: 0.8363636\n",
      "epoch: 633  -  cost: 0.4580565 - MSE:  8.733990976415713 -Train Accuracy: 0.76363635\n",
      "epoch: 634  -  cost: 0.39931187 - MSE:  8.716117003756798 -Train Accuracy: 0.8363636\n",
      "epoch: 635  -  cost: 0.45699129 - MSE:  8.749254773339581 -Train Accuracy: 0.76363635\n",
      "epoch: 636  -  cost: 0.39808285 - MSE:  8.727737388514166 -Train Accuracy: 0.8363636\n",
      "epoch: 637  -  cost: 0.4559427 - MSE:  8.764248143743945 -Train Accuracy: 0.76969695\n",
      "epoch: 638  -  cost: 0.3968655 - MSE:  8.739000499868345 -Train Accuracy: 0.8363636\n",
      "epoch: 639  -  cost: 0.45490944 - MSE:  8.778950400343836 -Train Accuracy: 0.76969695\n",
      "epoch: 640  -  cost: 0.39565897 - MSE:  8.749903506695704 -Train Accuracy: 0.8363636\n",
      "epoch: 641  -  cost: 0.45388997 - MSE:  8.793343667195254 -Train Accuracy: 0.76969695\n",
      "epoch: 642  -  cost: 0.39446282 - MSE:  8.760450543147858 -Train Accuracy: 0.8363636\n",
      "epoch: 643  -  cost: 0.45288262 - MSE:  8.807407002276424 -Train Accuracy: 0.76969695\n",
      "epoch: 644  -  cost: 0.39327657 - MSE:  8.770640288764238 -Train Accuracy: 0.8363636\n",
      "epoch: 645  -  cost: 0.4518862 - MSE:  8.821128891119619 -Train Accuracy: 0.76969695\n",
      "epoch: 646  -  cost: 0.39209965 - MSE:  8.780478327994599 -Train Accuracy: 0.8363636\n",
      "epoch: 647  -  cost: 0.4508989 - MSE:  8.83449442908479 -Train Accuracy: 0.76969695\n",
      "epoch: 648  -  cost: 0.39093116 - MSE:  8.789972353027938 -Train Accuracy: 0.8363636\n",
      "epoch: 649  -  cost: 0.44991875 - MSE:  8.847495236195542 -Train Accuracy: 0.76969695\n",
      "epoch: 650  -  cost: 0.3897705 - MSE:  8.799129076637703 -Train Accuracy: 0.8363636\n",
      "epoch: 651  -  cost: 0.44894436 - MSE:  8.860124318847117 -Train Accuracy: 0.76969695\n",
      "epoch: 652  -  cost: 0.38861707 - MSE:  8.807959790668598 -Train Accuracy: 0.8363636\n",
      "epoch: 653  -  cost: 0.44797432 - MSE:  8.872382892842166 -Train Accuracy: 0.76969695\n",
      "epoch: 654  -  cost: 0.3874702 - MSE:  8.816474351640581 -Train Accuracy: 0.8363636\n",
      "epoch: 655  -  cost: 0.44700673 - MSE:  8.884267225537393 -Train Accuracy: 0.76969695\n",
      "epoch: 656  -  cost: 0.38632905 - MSE:  8.824683126633417 -Train Accuracy: 0.8363636\n",
      "epoch: 657  -  cost: 0.44604045 - MSE:  8.89578210234777 -Train Accuracy: 0.77575755\n",
      "epoch: 658  -  cost: 0.38519314 - MSE:  8.832598441192541 -Train Accuracy: 0.8363636\n",
      "epoch: 659  -  cost: 0.44507387 - MSE:  8.906933058363734 -Train Accuracy: 0.7818182\n",
      "epoch: 660  -  cost: 0.3840619 - MSE:  8.840232352179052 -Train Accuracy: 0.8363636\n",
      "epoch: 661  -  cost: 0.44410658 - MSE:  8.9177293868352 -Train Accuracy: 0.7878788\n",
      "epoch: 662  -  cost: 0.38293514 - MSE:  8.847596493630991 -Train Accuracy: 0.830303\n",
      "epoch: 663  -  cost: 0.44313776 - MSE:  8.92818192890326 -Train Accuracy: 0.7878788\n",
      "epoch: 664  -  cost: 0.38181213 - MSE:  8.854701661065711 -Train Accuracy: 0.830303\n",
      "epoch: 665  -  cost: 0.442166 - MSE:  8.938301721811545 -Train Accuracy: 0.7878788\n",
      "epoch: 666  -  cost: 0.38069266 - MSE:  8.861559086522085 -Train Accuracy: 0.830303\n",
      "epoch: 667  -  cost: 0.44119155 - MSE:  8.94810359920666 -Train Accuracy: 0.7878788\n",
      "epoch: 668  -  cost: 0.37957656 - MSE:  8.868177587087203 -Train Accuracy: 0.8363636\n",
      "epoch: 669  -  cost: 0.4402139 - MSE:  8.957600166315373 -Train Accuracy: 0.7878788\n",
      "epoch: 670  -  cost: 0.37846348 - MSE:  8.874565452395059 -Train Accuracy: 0.8363636\n",
      "epoch: 671  -  cost: 0.43923235 - MSE:  8.966806108682192 -Train Accuracy: 0.7878788\n",
      "epoch: 672  -  cost: 0.3773534 - MSE:  8.880732398766416 -Train Accuracy: 0.8363636\n",
      "epoch: 673  -  cost: 0.43824717 - MSE:  8.97573786384589 -Train Accuracy: 0.7878788\n",
      "epoch: 674  -  cost: 0.3762461 - MSE:  8.886684274859206 -Train Accuracy: 0.8363636\n",
      "epoch: 675  -  cost: 0.43725806 - MSE:  8.984409749113215 -Train Accuracy: 0.7878788\n",
      "epoch: 676  -  cost: 0.37514117 - MSE:  8.892424475608964 -Train Accuracy: 0.8363636\n",
      "epoch: 677  -  cost: 0.43626478 - MSE:  8.992835411989983 -Train Accuracy: 0.7878788\n",
      "epoch: 678  -  cost: 0.37403902 - MSE:  8.897962418419626 -Train Accuracy: 0.8363636\n",
      "epoch: 679  -  cost: 0.43526793 - MSE:  9.001029258432382 -Train Accuracy: 0.7878788\n",
      "epoch: 680  -  cost: 0.37293935 - MSE:  8.90329843866804 -Train Accuracy: 0.8363636\n",
      "epoch: 681  -  cost: 0.43426713 - MSE:  9.009004146542102 -Train Accuracy: 0.7878788\n",
      "epoch: 682  -  cost: 0.3718422 - MSE:  8.908436557224737 -Train Accuracy: 0.8363636\n",
      "epoch: 683  -  cost: 0.43326253 - MSE:  9.016774347132456 -Train Accuracy: 0.7878788\n",
      "epoch: 684  -  cost: 0.3707474 - MSE:  8.913379307447267 -Train Accuracy: 0.8363636\n",
      "epoch: 685  -  cost: 0.43225417 - MSE:  9.024349754861042 -Train Accuracy: 0.7878788\n",
      "epoch: 686  -  cost: 0.36965483 - MSE:  8.918130066214692 -Train Accuracy: 0.8363636\n",
      "epoch: 687  -  cost: 0.43124232 - MSE:  9.031742630067978 -Train Accuracy: 0.7878788\n",
      "epoch: 688  -  cost: 0.3685646 - MSE:  8.922687742795446 -Train Accuracy: 0.8424242\n",
      "epoch: 689  -  cost: 0.4302265 - MSE:  9.038959668947902 -Train Accuracy: 0.7939394\n",
      "epoch: 690  -  cost: 0.36747646 - MSE:  8.927052363882122 -Train Accuracy: 0.8484849\n",
      "epoch: 691  -  cost: 0.42920727 - MSE:  9.046013914616184 -Train Accuracy: 0.7939394\n",
      "epoch: 692  -  cost: 0.36639062 - MSE:  8.931226126587376 -Train Accuracy: 0.8545455\n",
      "epoch: 693  -  cost: 0.42818478 - MSE:  9.052911618474562 -Train Accuracy: 0.7939394\n",
      "epoch: 694  -  cost: 0.36530688 - MSE:  8.935206976315106 -Train Accuracy: 0.8545455\n",
      "epoch: 695  -  cost: 0.4271587 - MSE:  9.059660159063737 -Train Accuracy: 0.7939394\n",
      "epoch: 696  -  cost: 0.3642252 - MSE:  8.938995873616241 -Train Accuracy: 0.8545455\n",
      "epoch: 697  -  cost: 0.42612928 - MSE:  9.066265065103586 -Train Accuracy: 0.7939394\n",
      "epoch: 698  -  cost: 0.36314574 - MSE:  8.94258927584265 -Train Accuracy: 0.8545455\n",
      "epoch: 699  -  cost: 0.42509678 - MSE:  9.072735234430914 -Train Accuracy: 0.7939394\n",
      "epoch: 700  -  cost: 0.36206812 - MSE:  8.945991372425922 -Train Accuracy: 0.8545455\n",
      "epoch: 701  -  cost: 0.4240608 - MSE:  9.079073004134111 -Train Accuracy: 0.7939394\n",
      "epoch: 702  -  cost: 0.36099237 - MSE:  8.94919492713949 -Train Accuracy: 0.8545455\n",
      "epoch: 703  -  cost: 0.42302153 - MSE:  9.085284290579434 -Train Accuracy: 0.7939394\n",
      "epoch: 704  -  cost: 0.35991845 - MSE:  8.952204489147782 -Train Accuracy: 0.8545455\n",
      "epoch: 705  -  cost: 0.42197886 - MSE:  9.09137223202794 -Train Accuracy: 0.7939394\n",
      "epoch: 706  -  cost: 0.35884616 - MSE:  8.955014495410788 -Train Accuracy: 0.8606061\n",
      "epoch: 707  -  cost: 0.42093265 - MSE:  9.097337061982339 -Train Accuracy: 0.7939394\n",
      "epoch: 708  -  cost: 0.35777554 - MSE:  8.957623799566395 -Train Accuracy: 0.8606061\n",
      "epoch: 709  -  cost: 0.41988304 - MSE:  9.10318375759909 -Train Accuracy: 0.7939394\n",
      "epoch: 710  -  cost: 0.35670644 - MSE:  8.960033389490517 -Train Accuracy: 0.8606061\n",
      "epoch: 711  -  cost: 0.4188299 - MSE:  9.108914459184026 -Train Accuracy: 0.7939394\n",
      "epoch: 712  -  cost: 0.35563877 - MSE:  8.962242688497303 -Train Accuracy: 0.8606061\n",
      "epoch: 713  -  cost: 0.41777316 - MSE:  9.11452881892599 -Train Accuracy: 0.7939394\n",
      "epoch: 714  -  cost: 0.35457247 - MSE:  8.96424655902628 -Train Accuracy: 0.8606061\n",
      "epoch: 715  -  cost: 0.41671282 - MSE:  9.120028460187841 -Train Accuracy: 0.7939394\n",
      "epoch: 716  -  cost: 0.35350743 - MSE:  8.966045058321518 -Train Accuracy: 0.8606061\n",
      "epoch: 717  -  cost: 0.41564858 - MSE:  9.125414164274716 -Train Accuracy: 0.7939394\n",
      "epoch: 718  -  cost: 0.35244334 - MSE:  8.96763722619123 -Train Accuracy: 0.8606061\n",
      "epoch: 719  -  cost: 0.4145801 - MSE:  9.13068425092014 -Train Accuracy: 0.7939394\n",
      "epoch: 720  -  cost: 0.35138014 - MSE:  8.969024487691115 -Train Accuracy: 0.8606061\n",
      "epoch: 721  -  cost: 0.41350755 - MSE:  9.13584127736568 -Train Accuracy: 0.8\n",
      "epoch: 722  -  cost: 0.3503177 - MSE:  8.970202839607664 -Train Accuracy: 0.8666667\n",
      "epoch: 723  -  cost: 0.41243058 - MSE:  9.140883038471236 -Train Accuracy: 0.8\n",
      "epoch: 724  -  cost: 0.34925607 - MSE:  8.971176333681948 -Train Accuracy: 0.8666667\n",
      "epoch: 725  -  cost: 0.41134953 - MSE:  9.145810901960251 -Train Accuracy: 0.8\n",
      "epoch: 726  -  cost: 0.34819478 - MSE:  8.97193948990209 -Train Accuracy: 0.8666667\n",
      "epoch: 727  -  cost: 0.41026348 - MSE:  9.150621931336103 -Train Accuracy: 0.8\n",
      "epoch: 728  -  cost: 0.34713376 - MSE:  8.972494032257128 -Train Accuracy: 0.8666667\n",
      "epoch: 729  -  cost: 0.40917242 - MSE:  9.15531448847314 -Train Accuracy: 0.8\n",
      "epoch: 730  -  cost: 0.34607288 - MSE:  8.972840568413185 -Train Accuracy: 0.8666667\n",
      "epoch: 731  -  cost: 0.40807617 - MSE:  9.159889298762899 -Train Accuracy: 0.8\n",
      "epoch: 732  -  cost: 0.3450119 - MSE:  8.972978203907534 -Train Accuracy: 0.8666667\n",
      "epoch: 733  -  cost: 0.40697455 - MSE:  9.164344468165986 -Train Accuracy: 0.8\n",
      "epoch: 734  -  cost: 0.3439507 - MSE:  8.972907343267716 -Train Accuracy: 0.8666667\n",
      "epoch: 735  -  cost: 0.40586725 - MSE:  9.168679166004145 -Train Accuracy: 0.8\n",
      "epoch: 736  -  cost: 0.34288913 - MSE:  8.972632663524628 -Train Accuracy: 0.8666667\n",
      "epoch: 737  -  cost: 0.4047543 - MSE:  9.172891649142487 -Train Accuracy: 0.8\n",
      "epoch: 738  -  cost: 0.34182677 - MSE:  8.972149760985566 -Train Accuracy: 0.8666667\n",
      "epoch: 739  -  cost: 0.4036347 - MSE:  9.176981430574662 -Train Accuracy: 0.8\n",
      "epoch: 740  -  cost: 0.34076348 - MSE:  8.971465408302649 -Train Accuracy: 0.8666667\n",
      "epoch: 741  -  cost: 0.4025085 - MSE:  9.180947817880519 -Train Accuracy: 0.8\n",
      "epoch: 742  -  cost: 0.33969903 - MSE:  8.97057746986209 -Train Accuracy: 0.8666667\n",
      "epoch: 743  -  cost: 0.40137547 - MSE:  9.184787846198263 -Train Accuracy: 0.8\n",
      "epoch: 744  -  cost: 0.3386333 - MSE:  8.969489689934145 -Train Accuracy: 0.8666667\n",
      "epoch: 745  -  cost: 0.4002355 - MSE:  9.188503472417056 -Train Accuracy: 0.8\n",
      "epoch: 746  -  cost: 0.3375659 - MSE:  8.96820329124084 -Train Accuracy: 0.8666667\n",
      "epoch: 747  -  cost: 0.3990876 - MSE:  9.192092033396388 -Train Accuracy: 0.8\n",
      "epoch: 748  -  cost: 0.33649668 - MSE:  8.966722389790293 -Train Accuracy: 0.8666667\n",
      "epoch: 749  -  cost: 0.3979321 - MSE:  9.195551536921286 -Train Accuracy: 0.8060606\n",
      "epoch: 750  -  cost: 0.33542526 - MSE:  8.96504623353996 -Train Accuracy: 0.8606061\n",
      "epoch: 751  -  cost: 0.39676818 - MSE:  9.198881174857185 -Train Accuracy: 0.8060606\n",
      "epoch: 752  -  cost: 0.33435163 - MSE:  8.963180953746551 -Train Accuracy: 0.8606061\n",
      "epoch: 753  -  cost: 0.39559576 - MSE:  9.20208457095637 -Train Accuracy: 0.8060606\n",
      "epoch: 754  -  cost: 0.33327532 - MSE:  8.961129910428395 -Train Accuracy: 0.8606061\n",
      "epoch: 755  -  cost: 0.39441437 - MSE:  9.205155338763987 -Train Accuracy: 0.8060606\n",
      "epoch: 756  -  cost: 0.3321961 - MSE:  8.958894710145325 -Train Accuracy: 0.8606061\n",
      "epoch: 757  -  cost: 0.3932236 - MSE:  9.208098302067429 -Train Accuracy: 0.8060606\n",
      "epoch: 758  -  cost: 0.33111373 - MSE:  8.956483251022048 -Train Accuracy: 0.8606061\n",
      "epoch: 759  -  cost: 0.39202303 - MSE:  9.210914089969801 -Train Accuracy: 0.8060606\n",
      "epoch: 760  -  cost: 0.33002797 - MSE:  8.953898122964404 -Train Accuracy: 0.8606061\n",
      "epoch: 761  -  cost: 0.39081255 - MSE:  9.213601136270702 -Train Accuracy: 0.8060606\n",
      "epoch: 762  -  cost: 0.32893863 - MSE:  8.951142554312941 -Train Accuracy: 0.8606061\n",
      "epoch: 763  -  cost: 0.38959157 - MSE:  9.216160817652293 -Train Accuracy: 0.8060606\n",
      "epoch: 764  -  cost: 0.32784528 - MSE:  8.948225830624672 -Train Accuracy: 0.8606061\n",
      "epoch: 765  -  cost: 0.38835976 - MSE:  9.218595853658396 -Train Accuracy: 0.8060606\n",
      "epoch: 766  -  cost: 0.32674775 - MSE:  8.945149880823445 -Train Accuracy: 0.8606061\n",
      "epoch: 767  -  cost: 0.38711688 - MSE:  9.22090368776002 -Train Accuracy: 0.8060606\n",
      "epoch: 768  -  cost: 0.32564577 - MSE:  8.94192268899695 -Train Accuracy: 0.8606061\n",
      "epoch: 769  -  cost: 0.38586244 - MSE:  9.223089923289868 -Train Accuracy: 0.8060606\n",
      "epoch: 770  -  cost: 0.32453907 - MSE:  8.93854843321301 -Train Accuracy: 0.8606061\n",
      "epoch: 771  -  cost: 0.3845962 - MSE:  9.225153343223502 -Train Accuracy: 0.8060606\n",
      "epoch: 772  -  cost: 0.32342744 - MSE:  8.935035524848635 -Train Accuracy: 0.8606061\n",
      "epoch: 773  -  cost: 0.38331753 - MSE:  9.227099023968098 -Train Accuracy: 0.8060606\n",
      "epoch: 774  -  cost: 0.32231057 - MSE:  8.93139188094001 -Train Accuracy: 0.8606061\n",
      "epoch: 775  -  cost: 0.3820267 - MSE:  9.228926762518327 -Train Accuracy: 0.8060606\n",
      "epoch: 776  -  cost: 0.32118833 - MSE:  8.927622206841502 -Train Accuracy: 0.8606061\n",
      "epoch: 777  -  cost: 0.3807229 - MSE:  9.230641681759023 -Train Accuracy: 0.8060606\n",
      "epoch: 778  -  cost: 0.3200605 - MSE:  8.923736411160231 -Train Accuracy: 0.8606061\n",
      "epoch: 779  -  cost: 0.37940624 - MSE:  9.232245964856762 -Train Accuracy: 0.8060606\n",
      "epoch: 780  -  cost: 0.31892702 - MSE:  8.919741322474355 -Train Accuracy: 0.8606061\n",
      "epoch: 781  -  cost: 0.37807667 - MSE:  9.23374251495423 -Train Accuracy: 0.8121212\n",
      "epoch: 782  -  cost: 0.31778768 - MSE:  8.915648329850047 -Train Accuracy: 0.8666667\n",
      "epoch: 783  -  cost: 0.37673387 - MSE:  9.235139832019762 -Train Accuracy: 0.8181818\n",
      "epoch: 784  -  cost: 0.3166422 - MSE:  8.911465474671646 -Train Accuracy: 0.8666667\n",
      "epoch: 785  -  cost: 0.37537694 - MSE:  9.236435365396302 -Train Accuracy: 0.8181818\n",
      "epoch: 786  -  cost: 0.31549022 - MSE:  8.907201135943493 -Train Accuracy: 0.8666667\n",
      "epoch: 787  -  cost: 0.37400624 - MSE:  9.237636877145242 -Train Accuracy: 0.8181818\n",
      "epoch: 788  -  cost: 0.31433183 - MSE:  8.902865553438769 -Train Accuracy: 0.8666667\n",
      "epoch: 789  -  cost: 0.37262163 - MSE:  9.238750600334178 -Train Accuracy: 0.8181818\n",
      "epoch: 790  -  cost: 0.31316715 - MSE:  8.898468745918544 -Train Accuracy: 0.8666667\n",
      "epoch: 791  -  cost: 0.37122372 - MSE:  9.23978252063761 -Train Accuracy: 0.8181818\n",
      "epoch: 792  -  cost: 0.3119961 - MSE:  8.894026576634811 -Train Accuracy: 0.8666667\n",
      "epoch: 793  -  cost: 0.36981198 - MSE:  9.240736037916955 -Train Accuracy: 0.8181818\n",
      "epoch: 794  -  cost: 0.31081846 - MSE:  8.8895440555092 -Train Accuracy: 0.8727273\n",
      "epoch: 795  -  cost: 0.36838606 - MSE:  9.241618227792918 -Train Accuracy: 0.8181818\n",
      "epoch: 796  -  cost: 0.30963403 - MSE:  8.88503527282044 -Train Accuracy: 0.8727273\n",
      "epoch: 797  -  cost: 0.3669463 - MSE:  9.242434585156689 -Train Accuracy: 0.8181818\n",
      "epoch: 798  -  cost: 0.30844328 - MSE:  8.880513228069972 -Train Accuracy: 0.8727273\n",
      "epoch: 799  -  cost: 0.36549315 - MSE:  9.243194040118178 -Train Accuracy: 0.8181818\n",
      "epoch: 800  -  cost: 0.30724597 - MSE:  8.875990241392023 -Train Accuracy: 0.8727273\n",
      "epoch: 801  -  cost: 0.36402673 - MSE:  9.243901773973235 -Train Accuracy: 0.8181818\n",
      "epoch: 802  -  cost: 0.3060426 - MSE:  8.871477699676017 -Train Accuracy: 0.8727273\n",
      "epoch: 803  -  cost: 0.36254764 - MSE:  9.244569781164179 -Train Accuracy: 0.8242424\n",
      "epoch: 804  -  cost: 0.3048331 - MSE:  8.866990047482208 -Train Accuracy: 0.8727273\n",
      "epoch: 805  -  cost: 0.36105588 - MSE:  9.24520213088517 -Train Accuracy: 0.8242424\n",
      "epoch: 806  -  cost: 0.3036177 - MSE:  8.862541669345156 -Train Accuracy: 0.8727273\n",
      "epoch: 807  -  cost: 0.3595516 - MSE:  9.245806967363645 -Train Accuracy: 0.8242424\n",
      "epoch: 808  -  cost: 0.3023964 - MSE:  8.858144266422448 -Train Accuracy: 0.8727273\n",
      "epoch: 809  -  cost: 0.35803548 - MSE:  9.246394388906557 -Train Accuracy: 0.8242424\n",
      "epoch: 810  -  cost: 0.3011697 - MSE:  8.8538130559721 -Train Accuracy: 0.8727273\n",
      "epoch: 811  -  cost: 0.35650817 - MSE:  9.246974084019676 -Train Accuracy: 0.830303\n",
      "epoch: 812  -  cost: 0.29993796 - MSE:  8.849563687316472 -Train Accuracy: 0.8727273\n",
      "epoch: 813  -  cost: 0.35497043 - MSE:  9.247554422283066 -Train Accuracy: 0.830303\n",
      "epoch: 814  -  cost: 0.29870147 - MSE:  8.845405536247126 -Train Accuracy: 0.8727273\n",
      "epoch: 815  -  cost: 0.35342258 - MSE:  9.248143742092203 -Train Accuracy: 0.8363636\n",
      "epoch: 816  -  cost: 0.29746082 - MSE:  8.841360484300708 -Train Accuracy: 0.8727273\n",
      "epoch: 817  -  cost: 0.35186586 - MSE:  9.248753086629725 -Train Accuracy: 0.8363636\n",
      "epoch: 818  -  cost: 0.29621616 - MSE:  8.837433934329908 -Train Accuracy: 0.8727273\n",
      "epoch: 819  -  cost: 0.3503002 - MSE:  9.249394137563982 -Train Accuracy: 0.8363636\n",
      "epoch: 820  -  cost: 0.294968 - MSE:  8.83364627305988 -Train Accuracy: 0.8727273\n",
      "epoch: 821  -  cost: 0.34872705 - MSE:  9.2500698964183 -Train Accuracy: 0.8363636\n",
      "epoch: 822  -  cost: 0.2937171 - MSE:  8.830006379891852 -Train Accuracy: 0.8727273\n",
      "epoch: 823  -  cost: 0.34714717 - MSE:  9.250796095933643 -Train Accuracy: 0.8363636\n",
      "epoch: 824  -  cost: 0.2924639 - MSE:  8.826535087644334 -Train Accuracy: 0.8727273\n",
      "epoch: 825  -  cost: 0.34556162 - MSE:  9.251583403553587 -Train Accuracy: 0.8363636\n",
      "epoch: 826  -  cost: 0.2912089 - MSE:  8.823239975367844 -Train Accuracy: 0.8727273\n",
      "epoch: 827  -  cost: 0.3439709 - MSE:  9.252439291522151 -Train Accuracy: 0.8363636\n",
      "epoch: 828  -  cost: 0.28995255 - MSE:  8.82013708122948 -Train Accuracy: 0.8727273\n",
      "epoch: 829  -  cost: 0.3423761 - MSE:  9.253374112300836 -Train Accuracy: 0.8363636\n",
      "epoch: 830  -  cost: 0.2886955 - MSE:  8.817237466617597 -Train Accuracy: 0.8787879\n",
      "epoch: 831  -  cost: 0.3407778 - MSE:  9.25439799434084 -Train Accuracy: 0.8363636\n",
      "epoch: 832  -  cost: 0.28743842 - MSE:  8.814556371384953 -Train Accuracy: 0.8787879\n",
      "epoch: 833  -  cost: 0.3391778 - MSE:  9.255519715582514 -Train Accuracy: 0.8363636\n",
      "epoch: 834  -  cost: 0.2861821 - MSE:  8.812104045589354 -Train Accuracy: 0.8787879\n",
      "epoch: 835  -  cost: 0.33757663 - MSE:  9.25675360348151 -Train Accuracy: 0.8424242\n",
      "epoch: 836  -  cost: 0.2849269 - MSE:  8.809889892980447 -Train Accuracy: 0.8787879\n",
      "epoch: 837  -  cost: 0.3359752 - MSE:  9.258103253447237 -Train Accuracy: 0.8424242\n",
      "epoch: 838  -  cost: 0.28367367 - MSE:  8.807926068385113 -Train Accuracy: 0.8787879\n",
      "epoch: 839  -  cost: 0.33437467 - MSE:  9.259579876749383 -Train Accuracy: 0.8424242\n",
      "epoch: 840  -  cost: 0.2824226 - MSE:  8.806221267336092 -Train Accuracy: 0.8787879\n",
      "epoch: 841  -  cost: 0.33277515 - MSE:  9.261190654847725 -Train Accuracy: 0.8424242\n",
      "epoch: 842  -  cost: 0.28117433 - MSE:  8.804785427073197 -Train Accuracy: 0.8848485\n",
      "epoch: 843  -  cost: 0.3311779 - MSE:  9.262942699119733 -Train Accuracy: 0.8424242\n",
      "epoch: 844  -  cost: 0.27992934 - MSE:  8.803625849579966 -Train Accuracy: 0.8848485\n",
      "epoch: 845  -  cost: 0.32958332 - MSE:  9.264845262496594 -Train Accuracy: 0.8424242\n",
      "epoch: 846  -  cost: 0.2786882 - MSE:  8.80275213773378 -Train Accuracy: 0.8848485\n",
      "epoch: 847  -  cost: 0.32799268 - MSE:  9.26690782743179 -Train Accuracy: 0.8424242\n",
      "epoch: 848  -  cost: 0.27745146 - MSE:  8.802170094858624 -Train Accuracy: 0.8848485\n",
      "epoch: 849  -  cost: 0.32640648 - MSE:  9.269133245995553 -Train Accuracy: 0.8424242\n",
      "epoch: 850  -  cost: 0.27621937 - MSE:  8.801883631721289 -Train Accuracy: 0.8848485\n",
      "epoch: 851  -  cost: 0.324825 - MSE:  9.271530454795828 -Train Accuracy: 0.8424242\n",
      "epoch: 852  -  cost: 0.27499223 - MSE:  8.80190106917616 -Train Accuracy: 0.8848485\n",
      "epoch: 853  -  cost: 0.3232488 - MSE:  9.274105589377418 -Train Accuracy: 0.8424242\n",
      "epoch: 854  -  cost: 0.27377057 - MSE:  8.802226161984615 -Train Accuracy: 0.8848485\n",
      "epoch: 855  -  cost: 0.3216783 - MSE:  9.276861613042021 -Train Accuracy: 0.8424242\n",
      "epoch: 856  -  cost: 0.27255446 - MSE:  8.802860708351048 -Train Accuracy: 0.8848485\n",
      "epoch: 857  -  cost: 0.32011425 - MSE:  9.279804279958745 -Train Accuracy: 0.8424242\n",
      "epoch: 858  -  cost: 0.2713444 - MSE:  8.80380637918797 -Train Accuracy: 0.8848485\n",
      "epoch: 859  -  cost: 0.31855682 - MSE:  9.282938766576684 -Train Accuracy: 0.8424242\n",
      "epoch: 860  -  cost: 0.27014044 - MSE:  8.805070269382817 -Train Accuracy: 0.8848485\n",
      "epoch: 861  -  cost: 0.3170061 - MSE:  9.28626948198278 -Train Accuracy: 0.8484849\n",
      "epoch: 862  -  cost: 0.26894268 - MSE:  8.806649900675907 -Train Accuracy: 0.8848485\n",
      "epoch: 863  -  cost: 0.31546223 - MSE:  9.289797671245429 -Train Accuracy: 0.8484849\n",
      "epoch: 864  -  cost: 0.26775143 - MSE:  8.80854723805455 -Train Accuracy: 0.8848485\n",
      "epoch: 865  -  cost: 0.31392556 - MSE:  9.29352631270821 -Train Accuracy: 0.8484849\n",
      "epoch: 866  -  cost: 0.2665666 - MSE:  8.810758843997458 -Train Accuracy: 0.8848485\n",
      "epoch: 867  -  cost: 0.31239602 - MSE:  9.297458524256744 -Train Accuracy: 0.8484849\n",
      "epoch: 868  -  cost: 0.2653882 - MSE:  8.813286853668368 -Train Accuracy: 0.8848485\n",
      "epoch: 869  -  cost: 0.31087345 - MSE:  9.301592847486855 -Train Accuracy: 0.8484849\n",
      "epoch: 870  -  cost: 0.2642163 - MSE:  8.816131620566559 -Train Accuracy: 0.8848485\n",
      "epoch: 871  -  cost: 0.30935833 - MSE:  9.305935981038358 -Train Accuracy: 0.8484849\n",
      "epoch: 872  -  cost: 0.2630511 - MSE:  8.819290471813165 -Train Accuracy: 0.8848485\n",
      "epoch: 873  -  cost: 0.3078505 - MSE:  9.310487813220425 -Train Accuracy: 0.8484849\n",
      "epoch: 874  -  cost: 0.26189247 - MSE:  8.822759189938441 -Train Accuracy: 0.8848485\n",
      "epoch: 875  -  cost: 0.30634993 - MSE:  9.315248808116653 -Train Accuracy: 0.8484849\n",
      "epoch: 876  -  cost: 0.26074037 - MSE:  8.826537628276434 -Train Accuracy: 0.8848485\n",
      "epoch: 877  -  cost: 0.30485642 - MSE:  9.32021828458186 -Train Accuracy: 0.8484849\n",
      "epoch: 878  -  cost: 0.25959462 - MSE:  8.830621892385588 -Train Accuracy: 0.8848485\n",
      "epoch: 879  -  cost: 0.30336982 - MSE:  9.325397015813131 -Train Accuracy: 0.8484849\n",
      "epoch: 880  -  cost: 0.258455 - MSE:  8.835007734171777 -Train Accuracy: 0.8848485\n",
      "epoch: 881  -  cost: 0.3018895 - MSE:  9.33078344791243 -Train Accuracy: 0.8545455\n",
      "epoch: 882  -  cost: 0.25732142 - MSE:  8.83969111193091 -Train Accuracy: 0.8848485\n",
      "epoch: 883  -  cost: 0.3004157 - MSE:  9.336375551289276 -Train Accuracy: 0.8545455\n",
      "epoch: 884  -  cost: 0.25619397 - MSE:  8.844669771630837 -Train Accuracy: 0.8848485\n",
      "epoch: 885  -  cost: 0.29894832 - MSE:  9.342175109065549 -Train Accuracy: 0.8545455\n",
      "epoch: 886  -  cost: 0.25507233 - MSE:  8.849937995973072 -Train Accuracy: 0.8848485\n",
      "epoch: 887  -  cost: 0.29748714 - MSE:  9.3481790431793 -Train Accuracy: 0.8545455\n",
      "epoch: 888  -  cost: 0.2539564 - MSE:  8.855490682559271 -Train Accuracy: 0.8848485\n",
      "epoch: 889  -  cost: 0.2960315 - MSE:  9.354386314164122 -Train Accuracy: 0.8545455\n",
      "epoch: 890  -  cost: 0.2528459 - MSE:  8.861323860299178 -Train Accuracy: 0.8848485\n",
      "epoch: 891  -  cost: 0.2945817 - MSE:  9.360793274135206 -Train Accuracy: 0.8545455\n",
      "epoch: 892  -  cost: 0.2517409 - MSE:  8.867432257875128 -Train Accuracy: 0.8848485\n",
      "epoch: 893  -  cost: 0.29313698 - MSE:  9.3674028919731 -Train Accuracy: 0.8545455\n",
      "epoch: 894  -  cost: 0.2506408 - MSE:  8.87381182923335 -Train Accuracy: 0.8848485\n",
      "epoch: 895  -  cost: 0.29169735 - MSE:  9.374207093666907 -Train Accuracy: 0.8545455\n",
      "epoch: 896  -  cost: 0.24954554 - MSE:  8.880452272920802 -Train Accuracy: 0.8848485\n",
      "epoch: 897  -  cost: 0.290262 - MSE:  9.381203540596845 -Train Accuracy: 0.8545455\n",
      "epoch: 898  -  cost: 0.24845481 - MSE:  8.887354084470381 -Train Accuracy: 0.8848485\n",
      "epoch: 899  -  cost: 0.288831 - MSE:  9.388389841478366 -Train Accuracy: 0.8545455\n",
      "epoch: 900  -  cost: 0.24736848 - MSE:  8.894509428268682 -Train Accuracy: 0.8909091\n",
      "epoch: 901  -  cost: 0.28740394 - MSE:  9.39576530047416 -Train Accuracy: 0.8545455\n",
      "epoch: 902  -  cost: 0.24628636 - MSE:  8.901912672348162 -Train Accuracy: 0.8909091\n",
      "epoch: 903  -  cost: 0.2859808 - MSE:  9.403326949416408 -Train Accuracy: 0.8545455\n",
      "epoch: 904  -  cost: 0.24520816 - MSE:  8.909558637012745 -Train Accuracy: 0.8909091\n",
      "epoch: 905  -  cost: 0.2845609 - MSE:  9.411067888157532 -Train Accuracy: 0.8545455\n",
      "epoch: 906  -  cost: 0.24413355 - MSE:  8.91744051882486 -Train Accuracy: 0.8909091\n",
      "epoch: 907  -  cost: 0.2831441 - MSE:  9.418985461508225 -Train Accuracy: 0.8545455\n",
      "epoch: 908  -  cost: 0.24306253 - MSE:  8.92555282798096 -Train Accuracy: 0.8909091\n",
      "epoch: 909  -  cost: 0.28173003 - MSE:  9.427077902810668 -Train Accuracy: 0.8545455\n",
      "epoch: 910  -  cost: 0.24199457 - MSE:  8.933891671585783 -Train Accuracy: 0.8909091\n",
      "epoch: 911  -  cost: 0.28031835 - MSE:  9.435341955910516 -Train Accuracy: 0.8545455\n",
      "epoch: 912  -  cost: 0.24092953 - MSE:  8.942448608490057 -Train Accuracy: 0.8909091\n",
      "epoch: 913  -  cost: 0.2789087 - MSE:  9.443771469474997 -Train Accuracy: 0.8545455\n",
      "epoch: 914  -  cost: 0.23986706 - MSE:  8.95121928104913 -Train Accuracy: 0.8909091\n",
      "epoch: 915  -  cost: 0.27750063 - MSE:  9.452361553800971 -Train Accuracy: 0.8545455\n",
      "epoch: 916  -  cost: 0.23880687 - MSE:  8.960195405784793 -Train Accuracy: 0.8909091\n",
      "epoch: 917  -  cost: 0.27609378 - MSE:  9.46110861600159 -Train Accuracy: 0.8545455\n",
      "epoch: 918  -  cost: 0.23774867 - MSE:  8.969375719708244 -Train Accuracy: 0.8909091\n",
      "epoch: 919  -  cost: 0.2746878 - MSE:  9.470008420399479 -Train Accuracy: 0.8545455\n",
      "epoch: 920  -  cost: 0.23669219 - MSE:  8.978750772318977 -Train Accuracy: 0.8909091\n",
      "epoch: 921  -  cost: 0.27328238 - MSE:  9.479056301770543 -Train Accuracy: 0.8545455\n",
      "epoch: 922  -  cost: 0.23563708 - MSE:  8.988315095097745 -Train Accuracy: 0.8969697\n",
      "epoch: 923  -  cost: 0.27187696 - MSE:  9.488248593781142 -Train Accuracy: 0.8545455\n",
      "epoch: 924  -  cost: 0.23458314 - MSE:  8.998065857537929 -Train Accuracy: 0.8969697\n",
      "epoch: 925  -  cost: 0.27047157 - MSE:  9.497584509117674 -Train Accuracy: 0.8545455\n",
      "epoch: 926  -  cost: 0.23353003 - MSE:  9.00799441007101 -Train Accuracy: 0.8969697\n",
      "epoch: 927  -  cost: 0.2690656 - MSE:  9.507052417351831 -Train Accuracy: 0.8545455\n",
      "epoch: 928  -  cost: 0.23247746 - MSE:  9.01809849457488 -Train Accuracy: 0.9030303\n",
      "epoch: 929  -  cost: 0.26765862 - MSE:  9.516651439910106 -Train Accuracy: 0.8545455\n",
      "epoch: 930  -  cost: 0.23142502 - MSE:  9.028368586524136 -Train Accuracy: 0.9030303\n",
      "epoch: 931  -  cost: 0.2662505 - MSE:  9.526375289906635 -Train Accuracy: 0.8545455\n",
      "epoch: 932  -  cost: 0.23037264 - MSE:  9.038802425366535 -Train Accuracy: 0.9030303\n",
      "epoch: 933  -  cost: 0.26484072 - MSE:  9.536223886946619 -Train Accuracy: 0.8545455\n",
      "epoch: 934  -  cost: 0.22931962 - MSE:  9.049393378968087 -Train Accuracy: 0.9030303\n",
      "epoch: 935  -  cost: 0.26342857 - MSE:  9.546186959422988 -Train Accuracy: 0.8545455\n",
      "epoch: 936  -  cost: 0.2282657 - MSE:  9.060135151200178 -Train Accuracy: 0.9030303\n",
      "epoch: 937  -  cost: 0.26201388 - MSE:  9.55625935433244 -Train Accuracy: 0.8545455\n",
      "epoch: 938  -  cost: 0.22721045 - MSE:  9.071022651814355 -Train Accuracy: 0.90909094\n",
      "epoch: 939  -  cost: 0.26059583 - MSE:  9.566439341634728 -Train Accuracy: 0.8545455\n",
      "epoch: 940  -  cost: 0.22615348 - MSE:  9.082054780551696 -Train Accuracy: 0.90909094\n",
      "epoch: 941  -  cost: 0.25917438 - MSE:  9.576721539544454 -Train Accuracy: 0.8545455\n",
      "epoch: 942  -  cost: 0.22509456 - MSE:  9.09322386419125 -Train Accuracy: 0.90909094\n",
      "epoch: 943  -  cost: 0.25774914 - MSE:  9.587100119260146 -Train Accuracy: 0.8545455\n",
      "epoch: 944  -  cost: 0.22403327 - MSE:  9.104522645905982 -Train Accuracy: 0.90909094\n",
      "epoch: 945  -  cost: 0.25631955 - MSE:  9.597571593608873 -Train Accuracy: 0.8545455\n",
      "epoch: 946  -  cost: 0.22296922 - MSE:  9.11594919166446 -Train Accuracy: 0.90909094\n",
      "epoch: 947  -  cost: 0.2548852 - MSE:  9.608129047432518 -Train Accuracy: 0.8545455\n",
      "epoch: 948  -  cost: 0.22190203 - MSE:  9.127495036024001 -Train Accuracy: 0.90909094\n",
      "epoch: 949  -  cost: 0.25344568 - MSE:  9.618769113271295 -Train Accuracy: 0.8606061\n",
      "epoch: 950  -  cost: 0.22083119 - MSE:  9.139161836452965 -Train Accuracy: 0.91515154\n",
      "epoch: 951  -  cost: 0.25200045 - MSE:  9.629488064441668 -Train Accuracy: 0.8606061\n",
      "epoch: 952  -  cost: 0.21975623 - MSE:  9.150938769416424 -Train Accuracy: 0.91515154\n",
      "epoch: 953  -  cost: 0.25054887 - MSE:  9.640281068620233 -Train Accuracy: 0.8606061\n",
      "epoch: 954  -  cost: 0.21867667 - MSE:  9.162824535725427 -Train Accuracy: 0.91515154\n",
      "epoch: 955  -  cost: 0.24909039 - MSE:  9.651134862163193 -Train Accuracy: 0.8606061\n",
      "epoch: 956  -  cost: 0.21759193 - MSE:  9.174809298476838 -Train Accuracy: 0.91515154\n",
      "epoch: 957  -  cost: 0.24762435 - MSE:  9.662051178200278 -Train Accuracy: 0.8606061\n",
      "epoch: 958  -  cost: 0.21650143 - MSE:  9.18689470396938 -Train Accuracy: 0.91515154\n",
      "epoch: 959  -  cost: 0.24615012 - MSE:  9.673024900850578 -Train Accuracy: 0.8545455\n",
      "epoch: 960  -  cost: 0.21540459 - MSE:  9.199075314956092 -Train Accuracy: 0.91515154\n",
      "epoch: 961  -  cost: 0.24466689 - MSE:  9.68404869511154 -Train Accuracy: 0.8545455\n",
      "epoch: 962  -  cost: 0.21430075 - MSE:  9.211343056258878 -Train Accuracy: 0.91515154\n",
      "epoch: 963  -  cost: 0.24317405 - MSE:  9.695116868528999 -Train Accuracy: 0.8545455\n",
      "epoch: 964  -  cost: 0.21318926 - MSE:  9.2236956454414 -Train Accuracy: 0.91515154\n",
      "epoch: 965  -  cost: 0.2416708 - MSE:  9.70622348901334 -Train Accuracy: 0.8545455\n",
      "epoch: 966  -  cost: 0.21206951 - MSE:  9.236126279954663 -Train Accuracy: 0.92121214\n",
      "epoch: 967  -  cost: 0.24015635 - MSE:  9.717362788132466 -Train Accuracy: 0.8545455\n",
      "epoch: 968  -  cost: 0.21094069 - MSE:  9.24863321794885 -Train Accuracy: 0.92121214\n",
      "epoch: 969  -  cost: 0.23863003 - MSE:  9.728529601014989 -Train Accuracy: 0.8545455\n",
      "epoch: 970  -  cost: 0.20980223 - MSE:  9.26121212888141 -Train Accuracy: 0.92121214\n",
      "epoch: 971  -  cost: 0.23709098 - MSE:  9.739721311448276 -Train Accuracy: 0.8545455\n",
      "epoch: 972  -  cost: 0.2086532 - MSE:  9.273858228018176 -Train Accuracy: 0.92121214\n",
      "epoch: 973  -  cost: 0.23553836 - MSE:  9.750930548078232 -Train Accuracy: 0.8606061\n",
      "epoch: 974  -  cost: 0.20749299 - MSE:  9.286565075855178 -Train Accuracy: 0.92121214\n",
      "epoch: 975  -  cost: 0.23397136 - MSE:  9.762149085920598 -Train Accuracy: 0.8606061\n",
      "epoch: 976  -  cost: 0.20632066 - MSE:  9.299330302011422 -Train Accuracy: 0.92121214\n",
      "epoch: 977  -  cost: 0.23238891 - MSE:  9.773375581016854 -Train Accuracy: 0.8606061\n",
      "epoch: 978  -  cost: 0.2051351 - MSE:  9.312147306505764 -Train Accuracy: 0.92121214\n",
      "epoch: 979  -  cost: 0.23078965 - MSE:  9.784598480809635 -Train Accuracy: 0.8606061\n",
      "epoch: 980  -  cost: 0.20393522 - MSE:  9.32501143666713 -Train Accuracy: 0.92121214\n",
      "epoch: 981  -  cost: 0.22917213 - MSE:  9.79581131854321 -Train Accuracy: 0.8666667\n",
      "epoch: 982  -  cost: 0.20271972 - MSE:  9.337917615429516 -Train Accuracy: 0.93333334\n",
      "epoch: 983  -  cost: 0.22753486 - MSE:  9.807005481309938 -Train Accuracy: 0.8666667\n",
      "epoch: 984  -  cost: 0.20148708 - MSE:  9.350859490994118 -Train Accuracy: 0.93333334\n",
      "epoch: 985  -  cost: 0.22587593 - MSE:  9.81817202914563 -Train Accuracy: 0.8666667\n",
      "epoch: 986  -  cost: 0.20023584 - MSE:  9.363830346740025 -Train Accuracy: 0.93333334\n",
      "epoch: 987  -  cost: 0.22419345 - MSE:  9.82929810492135 -Train Accuracy: 0.8666667\n",
      "epoch: 988  -  cost: 0.19896397 - MSE:  9.376822663893549 -Train Accuracy: 0.93333334\n",
      "epoch: 989  -  cost: 0.22248468 - MSE:  9.84037601959667 -Train Accuracy: 0.8666667\n",
      "epoch: 990  -  cost: 0.1976694 - MSE:  9.389830078001818 -Train Accuracy: 0.93939394\n",
      "epoch: 991  -  cost: 0.22074722 - MSE:  9.851389114071926 -Train Accuracy: 0.8666667\n",
      "epoch: 992  -  cost: 0.19635001 - MSE:  9.40284242457028 -Train Accuracy: 0.93939394\n",
      "epoch: 993  -  cost: 0.21897797 - MSE:  9.862324229007006 -Train Accuracy: 0.8666667\n",
      "epoch: 994  -  cost: 0.1950028 - MSE:  9.415847140993666 -Train Accuracy: 0.93939394\n",
      "epoch: 995  -  cost: 0.21717337 - MSE:  9.87316322547467 -Train Accuracy: 0.8666667\n",
      "epoch: 996  -  cost: 0.19362487 - MSE:  9.428835839283606 -Train Accuracy: 0.93939394\n",
      "epoch: 997  -  cost: 0.21532942 - MSE:  9.883887430680344 -Train Accuracy: 0.8727273\n",
      "epoch: 998  -  cost: 0.19221273 - MSE:  9.44179362154347 -Train Accuracy: 0.93939394\n",
      "epoch: 999  -  cost: 0.21344131 - MSE:  9.894472890730455 -Train Accuracy: 0.8727273\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b5cb51d5289e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' - '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cost:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'- MSE: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmse_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-Train Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved in file: %s'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "mse_history=[]\n",
    "accuracy_history=[]\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    sess.run(training_step,feed_dict={x:train_x,y_:train_y})\n",
    "    cost=sess.run(cost_function,feed_dict={x:train_x,y_:train_y})\n",
    "    cost_history=np.append(cost_history,cost)\n",
    "    correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    pred_y=sess.run(y,feed_dict={x:test_x})\n",
    "    mse=tf.reduce_mean(tf.square(pred_y-test_y))\n",
    "    mse_=sess.run(mse)\n",
    "    mse_history.append(mse_)\n",
    "    accuracy=(sess.run(accuracy,feed_dict={x:train_x,y_:train_y}))\n",
    "    accuracy_history.append(accuracy)\n",
    "    \n",
    "    print('epoch:',epoch,' - ','cost:',cost,'- MSE: ',mse_,\"-Train Accuracy:\",accuracy)\n",
    "    \n",
    "save_path=saver.save(sess,model_path)\n",
    "print('Model saved in file: %s'% save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot mse and accuracy graph **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mse_history,'r')\n",
    "plt.show()\n",
    "plt.plot(accuracy_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the final accuracy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8095238\n"
     ]
    }
   ],
   "source": [
    "correct_pridiction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print(\"Test Accuracy:\",(sess.run(accuracy,feed_dict={x:test_x,y_:test_y})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the final mean Square error **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 9.8945\n"
     ]
    }
   ],
   "source": [
    "pred_y=sess.run(y,feed_dict={x: test_x})\n",
    "mse=tf.reduce_mean(tf.square(pred_y-test_y))\n",
    "print(\"MSE: %.4f\" % sess.run(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
